{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/frederik-kilpinen/ASDS2/blob/main/Notebooks/data_processing_bigrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TuJx9CUL0Kt"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymB9xub9L0K1"
   },
   "source": [
    "**By: Frederik, Connor, Matias, Lukas**\n",
    "\n",
    "This notebook contains all the data-processing steps taken before analysis is done. The data comes from two-sources:\n",
    "1. Meta Data about Australian parlamentarians(MPs) comes from the research project http://twitterpoliticians.org/download. We call this MP data.\n",
    "2. The latest 3200 tweets from Australian MPs that we have collected. We call this tweet data.\n",
    "\n",
    "In short we do the following preprocessing steps:\n",
    "\n",
    "1. Process the MP data by:\n",
    "    * subsetting relevant variables\n",
    "    * renaming Nick Xenophon Team to center alliance (its later name)\n",
    "    * removing titles such as Mr or Ms from MP names\n",
    "2. Merge the two data-sets\n",
    "3. Subset on MPs that were active MPs during the time of their tweet\n",
    "4. Subset on the time-period 1 year before the bushfire (1. June 2018) and 1 year after the bushfire (1. May 2021)\n",
    "5. clean the tweet text by:\n",
    "    * lower-casing the text\n",
    "    * remove special characters, punctuation, symbols, mentions, emojis\n",
    "    * remove english stop words (nltk)\n",
    "    * columns where we retain lemmas and stems\n",
    "    * columns where we retain part-of-speech from lemmas and stems\n",
    "\n",
    "\n",
    "The final data frame contains the following columns:\n",
    "\n",
    "* screen_name\n",
    "* user_id \n",
    "* tweet_id\n",
    "* created_at\n",
    "* full_text',\n",
    "* favorite_count\n",
    "* retweet_count\n",
    "* in_reply_to_screen_name',\n",
    "* hashtags\n",
    "* user_mentions\n",
    "* url\n",
    "* image_url\n",
    "* name\n",
    "* party\n",
    "* legislative_period\n",
    "* lemmas\n",
    "* stems\n",
    "* pos_lemmas \n",
    "* pos_stems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5jNeH3d9cqY",
    "outputId": "3b3cd7d6-614d-4094-db23-fd5ea14042b4"
   },
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#import tweepy\n",
    "from datetime import date\n",
    "import pickle \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# If google colab:\n",
    "#!pip install nltk==3.4\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "LiXo-A0o9cqd"
   },
   "outputs": [],
   "source": [
    "class DataProcessing:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Set the file path. Change if necessary\n",
    "        tweet_data_path = \"data/mp_tweets2.csv\"\n",
    "        mp_data_path = \"data/full_member_info.csv\"\n",
    "\n",
    "        self.tweet_data = pd.read_csv(tweet_data_path, index_col = 0)\n",
    "        self.mp_data = pd.read_csv(mp_data_path)\n",
    "    \n",
    "    \n",
    "    def compile_final_df(self):\n",
    "        \"\"\"\n",
    "        This method compiles the final data-set used in our analysis. Doing the following steps:\n",
    "            1. Loads and \n",
    "        \n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        #Clean the Twitter data\n",
    "        tweet_df = self.clean_tweet_data(self.tweet_data)\n",
    "        #Clean the mp_info data\n",
    "        mp_df = self.clean_mp_data(self.mp_data)\n",
    "        \n",
    "        print(\"-\"*66)\n",
    "        print(f\"Shape of twitter data: {tweet_df.shape}\\nShape of MP data: {mp_df.shape}\")\n",
    "        \n",
    "        #Merge to final df\n",
    "        final_df = tweet_df.merge(mp_df, on = \"user_id\", how = \"left\")\n",
    "        \n",
    "        #Subset on active MPs\n",
    "        final_df = final_df.loc[((final_df[\"legislative_period\"] == \"45\") & (final_df[\"created_at\"] < \"2019-07-01\"))|\n",
    "                                ((final_df[\"legislative_period\"] == \"46\") & (final_df[\"created_at\"] > \"2019-07-01\"))]\n",
    "        \n",
    "        #Subset tweets from 1 year before the bushfire (1. June 2018) and 1 year after the bushfire (1. May 2021)\n",
    "        final_df = final_df.loc[(final_df[\"created_at\"] >= \"2018-06-01\") & (final_df[\"created_at\"] <= \"2021-04-30\")]\n",
    "           \n",
    "        #Restetting index for final df\n",
    "        final_df = final_df.reset_index(drop = True)\n",
    "        \n",
    "        print(\"-\"*66)\n",
    "        print(f\"Shape of final data-frame: {final_df.shape}\" )\n",
    "        print(\"Time to execute: \", \"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        print(\"-\"*66)\n",
    "        print(\"Begining to process the tweet text. Restarting timer...\")\n",
    "        \n",
    "        #Get the stems and lemmas\n",
    "        final_df[\"stems\"] = final_df[\"full_text\"].apply(lambda tweet: self.preprocess_stem(tweet))\n",
    "\n",
    "        # Sreating stemmed bigrams\n",
    "        def join_tups(lst):\n",
    "            return ['_'.join(tup) for tup in lst]\n",
    "        final_df[\"stems_bigrams\"] = final_df[\"stems\"].apply(lambda x: list(nltk.bigrams(x)))\n",
    "        final_df[\"stems_bigrams\"] =  final_df[\"stems_bigrams\"].apply(lambda x: join_tups(x))\n",
    "        #final_df[\"stems\"] +\n",
    "        \n",
    "        print(\"-\"*66)\n",
    "        print(\"FINISHED: time to execute: \", \"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def clean_tweet_data(self, tweet_df):\n",
    "\n",
    "\n",
    "        #Drop 6 tweets that are corrupt. Because of it only being 6 tweets we drop them instead of re-running the collection from the API\n",
    "        remove_idx = [175522, 190414, 211953, 212012, 212013, 212298 ]\n",
    "        tweet_df = tweet_df.drop(tweet_df.index[remove_idx])\n",
    "\n",
    "        #Make data into date-time object, remove h-m-s from dt\n",
    "        tweet_df[\"created_at\"] = pd.to_datetime(pd.to_datetime(tweet_df[\"created_at\"]).dt.date)\n",
    "        \n",
    "        tweet_df[\"user_id\"] = tweet_df[\"user_id\"].astype(int)\n",
    "        \n",
    "        return tweet_df\n",
    "    \n",
    "    def clean_mp_data(self, mp_df):\n",
    "        \n",
    "        #Select relevant columns\n",
    "        mp_df = mp_df[['p.country', 'm.name', 'p.party', 'm.uid', 'lp.official_legislative_period']]\n",
    "        mp_df = mp_df.loc[mp_df[\"p.country\"]==\"Australia\"]\n",
    "        \n",
    "        #Drop australia column\n",
    "        mp_df = mp_df.drop(columns = [\"p.country\"])\n",
    "        #Rename some columns\n",
    "        mp_df = mp_df.rename(columns = {\"m.name\":\"name\", \"p.party\":\"party\",\n",
    "                                       \"lp.official_legislative_period\":\"legislative_period\"})\n",
    "        \n",
    "        #Rename user id column for merging with members_info data\n",
    "        mp_df = mp_df.rename(columns = {\"m.uid\":\"user_id\"})\n",
    "        \n",
    "        #remove titles from the names\n",
    "        remove = r\"(^[A-Za-z]{2}\\s{1}|\\s{1}[A-Z]{2,}|^Hon\\s{1}|^Mrs\\s{1}|(Dr\\s)|,)\"\n",
    "        mp_df[\"name\"] = mp_df[\"name\"].str.replace(remove, \"\", regex = True)\n",
    "        \n",
    "        #remove mps that don't have twitter\n",
    "        mp_df = mp_df.loc[mp_df[\"user_id\"] != \"\\\\N\"]\n",
    "        mp_df[\"user_id\"] = mp_df[\"user_id\"].astype(int)\n",
    "        \n",
    "        # Merge the Nick Xenophon Team and Centre Alliance \n",
    "        mp_df[\"party\"] = mp_df[\"party\"].apply(lambda x: \"Centre Alliance\" if x == \"Nick Xenophon Team\" else x)\n",
    "        \n",
    "        return mp_df\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "\n",
    "        #Lowercasing words\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        text = re.sub(r'…', '', text)\n",
    "\n",
    "        #Removing '&amp' which was found to be common\n",
    "        text = re.sub(r'&amp','', text)\n",
    "\n",
    "        #Replace other instances of \"&\" with \"and\"\n",
    "        text = re.sub(r'&','and', text)\n",
    "\n",
    "        #Removing mentions \n",
    "        text = re.sub(r'@\\w+ ', '', text)\n",
    "\n",
    "        #Removing 'RT' and 'via'\n",
    "        text = re.sub(r'(^rt|^via)((?:\\b\\W*@\\w+)+): ', '', text)\n",
    "\n",
    "        #Removing emojis\n",
    "        EMOJI_PATTERN = re.compile(\n",
    "          \"[\"\n",
    "          \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "          \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "          \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "          \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "          \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "          \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "          \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "          \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "          \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "          \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "          \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "          \"\\U000024C2-\\U0001F251\" \n",
    "          \"]+\"\n",
    "          )\n",
    "        \n",
    "        text = re.sub(EMOJI_PATTERN, '', text)\n",
    "\n",
    "        #Removing punctuation\n",
    "        my_punctuation = string.punctuation.replace('#','')\n",
    "        my_punctuation = my_punctuation.replace('-','')\n",
    "\n",
    "        text = text.translate(str.maketrans('', '', my_punctuation))\n",
    "        text = re.sub(r' - ','', text) #removing dash lines bounded by whitespace (and therefore not part of a word)\n",
    "        text = re.sub(r'[’“”—,!]','',text) #removing punctuation that is not captured by string.punctuation\n",
    "\n",
    "        #Removing odd special characters\n",
    "        text = re.sub(r\"[┻┃━┳┓┏┛┗]\",\"\", text)\n",
    "        text = re.sub(r\"\\u202F|\\u2069|\\u200d|\\u2066\",\"\", text)\n",
    "\n",
    "        #Removing URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        #Removing numbers\n",
    "        text = re.sub(r'[0-9]','', text)\n",
    "\n",
    "        #Removing separators and superfluous whitespace\n",
    "        text = text.strip()\n",
    "        text = re.sub(r' +',' ',text)\n",
    "\n",
    "        #Tokenizing\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def preprocess_stem(self, tokens):\n",
    "\n",
    "        #Running the preprocess function\n",
    "        tokens = self.preprocess_text(tokens)\n",
    "\n",
    "        #Removing stopwords\n",
    "        stop_words_list = nltk.corpus.stopwords.words(\"english\")\n",
    "        tokens = [i for i in tokens if i not in stop_words_list]\n",
    "\n",
    "        #Stemming\n",
    "        stemmer = nltk.PorterStemmer()    #Creating stemmer\n",
    "        sent_stemmed = []                 #Empty list to save stemmed sentence\n",
    "\n",
    "        for word in tokens:\n",
    "            stem = stemmer.stem(word)     #Stemming words\n",
    "            sent_stemmed.append(stem)\n",
    "\n",
    "        tokens = sent_stemmed\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "OiQmYnQI9cqe",
    "outputId": "8ce013cb-dedc-4466-8c9e-2601550e774a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matiasp/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3357: DtypeWarning: Columns (0,3,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "Shape of twitter data: (335969, 13)\n",
      "Shape of MP data: (258, 4)\n",
      "------------------------------------------------------------------\n",
      "Shape of final data-frame: (170338, 16)\n",
      "Time to execute:  --- 0.9695992469787598 seconds ---\n",
      "------------------------------------------------------------------\n",
      "Begining to process the tweet text. Restarting timer...\n",
      "------------------------------------------------------------------\n",
      "FINISHED: time to execute:  --- 104.73382997512817 seconds ---\n"
     ]
    }
   ],
   "source": [
    "processor = DataProcessing()\n",
    "final_df = processor.compile_final_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zTx6WmcL0LE",
    "outputId": "f6c3dddb-940b-4f2c-c7b4-dbd8247638b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170338, 17)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"final_text\"] = final_df[\"stems\"].str.join(\" \") + final_df[\"stems_bigrams\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mid-year econom fiscal outlook confirm budget return surplu - economi growmid-year_econom econom_fiscal fiscal_outlook outlook_confirm confirm_budget budget_return return_surplu surplu_- -_economi economi_grow'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"final_text\"][29009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "_fB8sfnWL0LG",
    "outputId": "7688526b-a42e-442f-90fb-f104e7a8866b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great support higher educ support packaggreat_support support_higher higher_educ educ_support support_packag\n"
     ]
    }
   ],
   "source": [
    "print(final_df[\"final_text\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "HT50yqLvL0LH"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv(\"data/final_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "data_processing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
