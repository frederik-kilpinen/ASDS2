{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.keyword_subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the output from the topic model and our immersion journal/manual generate new keywords through computationally assited keyword retrival. Using a rich set of qualitatively evaluated keywords we then define a subset of bushfire tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import pandas as pd\n",
    "#Word embeddings\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import PorterStemmer\n",
    "#Gary King et. al key-words\n",
    "from keyword_algorithm import *\n",
    "#Remove unwarranted warnings\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gary Kings et. al. (2017) computationally assited keyword retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the algorithm introduced Gary King et. al. (2017) with a pre-trained word embeddings model. We set it up such that the algorithm is run iteratively for a selected amount of times. For the keyword algorithm we use the replication material code named ```keyword_algorithm.py```and is provided here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FMJDCD. We also had to update some functions to be compatible with the latest version of Pandas. To display our workflow we go through 1 iteration bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and prepare data for the model\n",
    "data = pd.read_csv(\"data/final_df.csv\", index_col=0)\n",
    "data = data.dropna(subset = [\"final_text\"]).reset_index(drop = True)\n",
    "data[\"index_col\"] = data.index\n",
    "data.to_csv(\"data/query_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download model from https://fasttext.cc/docs/en/english-vectors.html\n",
    "model_dir = \"data/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    \n",
    "    def __init__(self, emb_model):\n",
    "        \n",
    "        self.query = Keywords()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        #Load the data. Change path if necessary\n",
    "        path = 'data/query_df.csv'\n",
    "        self.query.LoadDataset(path, text_colname='final_text', \n",
    "                    date_colname=\"created_at\", id_colname=\"index_col\")\n",
    "        #Pre-trained word embeddings model \n",
    "        self.we = emb_model\n",
    "            \n",
    "    def get_keywords(self, its = 2, top_n = 10, refkeys = [], tarkeys = [], algorithms = ['nbayes', 'logit'], \n",
    "                     date_start = \"2019-06-01\", date_end = \"2020-05-30\"):\n",
    "        \"\"\"\n",
    "        Loops over King. et. al algorithm to extract relevant keywords used for building a \n",
    "        boolean query to subset relevant Tweets in the dataset.\n",
    "        ---------\n",
    "        arguments:\n",
    "            - its: Iterations to run the algorithm\n",
    "            - top_n: integer of how many of the most predictive keywords to extract in each iteration\n",
    "            - refkeys: list of initial keywords used to create reference set of tweets\n",
    "            - tarkeys: list of initial keywords used to limit the search set\n",
    "            - algorithms: list of classifiers to run for extracting keywords \n",
    "            - date_start: y/m/d of start date for relevant tweets\n",
    "            - date_end: y/m/d of end date for relevant tweets\n",
    "        -----------    \n",
    "        returns:\n",
    "            - dictionary of accepted, rejected and nontarget keywords\n",
    "            - the trained query model object\n",
    "        \"\"\"\n",
    "        \n",
    "        accepted_keywords = []\n",
    "        rejected_keywords = []\n",
    "        nontarget_keywords = []\n",
    "        \n",
    "        #Begin loop for mining search set\n",
    "        for it in range(its):\n",
    "            print(\"-\"*66)\n",
    "            print(f\"STARTING ITERATION: {it}!\")\n",
    "            if it == 0:\n",
    "                print(f\"INITIAL REFERENCE KEYS: {refkeys} \\n INITIAL TARGET KEYS: {tarkeys}\")\n",
    "            print(\"-\"*66)\n",
    "            \n",
    "            #Build reference set of tweets\n",
    "            self.query.ReferenceSet(any_words=refkeys, date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            #Use accepted keys as search keys if not the first iteration\n",
    "            if it > 0:\n",
    "                self.query.SearchSet(any_words = accepted_keywords, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            else:\n",
    "                self.query.SearchSet(any_words = tarkeys, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            \n",
    "            #Run King algorithm to find keywords.\n",
    "            self.query.ProcessData(stem = False, keep_twitter_symbols=False,\n",
    "                                   remove_wordlist=refkeys)\n",
    "            self.query.ReferenceKeywords()\n",
    "            self.query.ClassifyDocs(min_df=5, ref_trainprop=1, algorithms=algorithms)\n",
    "            self.query.FindTargetSet()\n",
    "            self.query.FindKeywords()\n",
    "            \n",
    "            #Extract target keywords from algorithm results\n",
    "            target_keywords = self.query.target_keywords[:top_n]\n",
    "            #Also get the reference set keywords to loop over\n",
    "            target_keywords += self.query.reference_keywords[:top_n]\n",
    "            #Append unique nontarget keywords to list of nontarget keys\n",
    "            for nonkey in self.query.nontarget_keywords[:100]:\n",
    "                if nonkey not in nontarget_keywords:\n",
    "                    nontarget_keywords.append(nonkey)\n",
    "            \n",
    "            #Loop over each relevant keyword from reference and found target keywords\n",
    "            for keyword in target_keywords:\n",
    "                #Check if keyword has already been rejected or accepted\n",
    "                if keyword in accepted_keywords or keyword in rejected_keywords:\n",
    "                    continue\n",
    "                else:\n",
    "                    inp = input(f\"Keep {keyword.upper()} yes or no?\")\n",
    "                    if inp == \"y\":\n",
    "                        accepted_keywords.append(keyword)\n",
    "                        #get similar keywords through most similar pretrained embeddings\n",
    "                        inp2 = input(f\"Look at {keyword.upper()}'s most similar word embeddings, yes or no?\")\n",
    "                        if inp2 == \"y\":\n",
    "                            #Look if keyword exist in embedding model dictionary\n",
    "                            try:\n",
    "                                #Get the stemmed embedding \n",
    "                                embeddings = [self.stemmer.stem(emb[0]) for emb in self.we.most_similar(keyword)]\n",
    "                                for emb in embeddings:\n",
    "                                    #Look if embedding already exist in embedding model dictionary\n",
    "                                    if emb.lower() in accepted_keywords or emb.lower() in rejected_keywords:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        inp3 = input(f\"Keep embedding {emb.upper()} yes or no?\")\n",
    "                                        if inp3 == \"y\":\n",
    "                                            accepted_keywords.append(emb)\n",
    "                                        elif inp3 ==\"n\":\n",
    "                                            rejected_keywords.append(emb)\n",
    "                            except:\n",
    "                                print(f\"{keyword.upper()} embedding not present in Model!\")\n",
    "                                pass\n",
    "                        elif inp2 == \"n\":\n",
    "                            pass\n",
    "                    elif inp == \"n\":\n",
    "                        rejected_keywords.append(keyword)\n",
    "                        \n",
    "            #Add custom keyword(s) in the end of the loop. Either as list or single keyword\n",
    "            inp4 = input(f\"Do you wish to add any further keywords? If yes, Type keyword: \")\n",
    "            if inp4:\n",
    "                if isinstance(inp4, list):\n",
    "                    [accepted_keywords.append(key) for key in inp4]\n",
    "                else:\n",
    "                     accepted_keywords.append(inp4)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            print(\"-\"*66)\n",
    "            print(\" \"*20, f\"CURRENT KEYWORDS AFTER ITTERATION {it}\")\n",
    "            print(\"-\"*66)\n",
    "            print(f\"ACCEPTED: \\n {accepted_keywords}\")\n",
    "            print(f\"REJECTED: \\n {rejected_keywords}\")\n",
    "            \n",
    "        fitted_model = self.query\n",
    "        keywords = {\"accepted_keys\":accepted_keywords, \"rejected_keys\":rejected_keywords,\"nontarget_keys\":nontarget_keywords}\n",
    "        \n",
    "        return keywords, fitted_model\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now demostrating the workflow for one iteration. Note that for each iteration the search set is normally expanded by the accepted keywords. We do this untill we see no significant increase in the amount of documents and we are not getting new keywords to either accept or reject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword object initialized.\n",
      "Loaded corpus of size 147440 in 4.02 seconds.\n",
      "------------------------------------------------------------------\n",
      "STARTING ITERATION: 0!\n",
      "INITIAL REFERENCE KEYS: ['bushfir|bushfir_crisi|bushfir_affect|firefight'] \n",
      " INITIAL TARGET KEYS: ['fire', 'disast', 'recov', 'emerg', 'wildlif', 'nsw']\n",
      "------------------------------------------------------------------\n",
      "Loaded reference set of size 1991 in 3.23 seconds.\n",
      "Loaded search set of size 4816 in 7.26 seconds.\n",
      "Time to process corpus: 2.3 seconds\n",
      "\n",
      "4199 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 6807 by 2595 with 92134 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.19 seconds\n",
      "\n",
      "Ref training size: 1991; Search training size: 1589; Training size: 3580; Test size: 4816\n",
      "\n",
      "Time for Naive Bayes: 0.0 seconds\n",
      "Time for Logit: 0.18 seconds\n",
      "1613 documents in target set\n",
      "3203 documents in non-target set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matiasp/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 target set keywords found\n",
      "709 non-target set keywords found\n",
      "Keep FIRE yes or no?y\n",
      "Look at FIRE's most similar word embeddings, yes or no?y\n",
      "Keep embedding FLAME yes or no?y\n",
      "Keep embedding THREE-ALARM yes or no?y\n",
      "Keep embedding TWO-ALARM yes or no?y\n",
      "Keep embedding FOUR-ALARM yes or no?y\n",
      "Keep embedding BLAZE yes or no?y\n",
      "Keep embedding FIVE-ALARM yes or no?y\n",
      "Keep embedding FIRE. yes or no?n\n",
      "Keep embedding CONFLAGR yes or no?y\n",
      "Keep AFFECT yes or no?n\n",
      "Keep RECOVERI yes or no?y\n",
      "Look at RECOVERI's most similar word embeddings, yes or no?y\n",
      "RECOVERI embedding not present in Model!\n",
      "Keep SUPPORT yes or no?y\n",
      "Look at SUPPORT's most similar word embeddings, yes or no?y\n",
      "Keep embedding SUPPPORT yes or no?n\n",
      "Keep embedding OPPOS yes or no?n\n",
      "Keep embedding BACK yes or no?n\n",
      "Keep embedding SUPORT yes or no?n\n",
      "Keep embedding SUPPRT yes or no?n\n",
      "Keep embedding HELP yes or no?y\n",
      "Keep DEVAST yes or no?y\n",
      "Look at DEVAST's most similar word embeddings, yes or no?n\n",
      "Keep SERVIC yes or no?y\n",
      "Look at SERVIC's most similar word embeddings, yes or no?y\n",
      "Keep embedding PBX yes or no?n\n",
      "Keep embedding SPECIALISTS. yes or no?n\n",
      "Keep embedding PROVIDER. yes or no?n\n",
      "Keep embedding UNSPECIFIED. yes or no?n\n",
      "Keep embedding COMPETITIVE. yes or no?n\n",
      "Keep embedding CONSULTANTS. yes or no?n\n",
      "Keep embedding CBINDUSTRI yes or no?n\n",
      "Keep embedding CDF. yes or no?n\n",
      "Keep embedding PROVI yes or no?n\n",
      "Keep embedding OLW. yes or no?n\n",
      "Keep SUMMER yes or no?y\n",
      "Look at SUMMER's most similar word embeddings, yes or no?n\n",
      "Keep THANK yes or no?n\n",
      "Keep AREA yes or no?n\n",
      "Keep COMMUN yes or no?n\n",
      "Keep TODAY yes or no?n\n",
      "Keep GOVERN yes or no?n\n",
      "Keep NEED yes or no?n\n",
      "Keep AUSTRALIA yes or no?n\n",
      "Do you wish to add any further keywords? If yes, Type keyword: koala\n",
      "------------------------------------------------------------------\n",
      "                     CURRENT KEYWORDS AFTER ITTERATION 0\n",
      "------------------------------------------------------------------\n",
      "ACCEPTED: \n",
      " ['fire', 'flame', 'three-alarm', 'two-alarm', 'four-alarm', 'blaze', 'five-alarm', 'conflagr', 'recoveri', 'support', 'help', 'devast', 'servic', 'summer', 'koala']\n",
      "REJECTED: \n",
      " ['fire.', 'affect', 'suppport', 'oppos', 'back', 'suport', 'supprt', 'pbx', 'specialists.', 'provider.', 'unspecified.', 'competitive.', 'consultants.', 'cbindustri', 'cdf.', 'provi', 'olw.', 'thank', 'area', 'commun', 'today', 'govern', 'need', 'australia']\n"
     ]
    }
   ],
   "source": [
    "#Initiate the query builder object\n",
    "query = QueryBuilder(fasttext)\n",
    "#Run the workflow using initial keywords found by our topic models to demarcate reference and search set\n",
    "keywords, model = query.get_keywords(its = 1, top_n=10, \n",
    "                                     refkeys=[\"bushfir|bushfir_crisi|bushfir_affect|firefight\"], \n",
    "                                     tarkeys=[\"fire\", \"disast\", \"recov\", \"emerg\",\"wildlif\", \"nsw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Bushfire Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on keywords found through the process above and our nethnography we define the subset of bushfire tweets bellow. Note that this is the final query which has been refined based on random sampling to and qualitative coding to evaluate accuracy as outlined in section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define keywords for subsetting\n",
    "positive_query = \"firey|bushfir|bushfir_crisi|bushfir_affect|firefight|fire|blaze|/^nsw$/|firefighters|firemen|conflagration|/^ash$/|smoke|aerial|opbushfireassist|burnings|burns|burnt|burned|burn|burning|nswfires|blacksummer|habitat|flames|two-alarm|three-alarm|four-alarm|five-alarm|blaze|fireman|habitats|wild-life\"\n",
    "negative_query = r\"covid|coron|covid|pandemic|epidemic|flu|rona|vaccin|virus\"\n",
    "positive_query = '|'.join(set([PorterStemmer().stem(w) for w in positive_query.split(\"|\")]))\n",
    "negative_query = '|'.join(set([PorterStemmer().stem(w) for w in negative_query.split(\"|\")]))\n",
    "\n",
    "#create subset\n",
    "subset = data.loc[(data[\"final_text\"].str.contains(positive_query))\n",
    "                 &(~data[\"final_text\"].str.contains(negative_query)) \n",
    "                 &(data[\"created_at\"] >= \"2019-06-01\") \n",
    "                 &(data[\"created_at\"] < \"2020-06-01\")]\n",
    "\n",
    "\n",
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bushfire dummy\n",
    "data[\"bushfire_dummy\"] = data[\"index_col\"].apply(lambda x: 1 if x in subset[\"index_col\"] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final_df with bushfire dummy\n",
    "data.to_csv(\"data/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save bushfire subset\n",
    "subset = subset.reset_index(drop = True)\n",
    "subset.to_csv(\"data/bushfire_subset.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd379f7bb8e06f7ec29e5b7f14bfd985640a300b5a41de1996cc2754af32f7b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
