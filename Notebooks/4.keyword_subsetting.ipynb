{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.keyword_subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the output from the topic model and our immersion journal/manual generate new keywords through computationally assited keyword retrival. Using a rich set of qualitatively evaluated keywords we then define a subset of bushfire tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import pandas as pd\n",
    "#Word embeddings\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import PorterStemmer\n",
    "#Gary King et. al key-words\n",
    "from keyword_algorithm import *\n",
    "#Remove unwarranted warnings\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gary Kings et. al. (2017) computationally assited keyword retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the algorithm introduced Gary King et. al. (2017) with a pre-trained word embeddings model. We set it up such that the algorithm is run iteratively for a selected amount of times. For the keyword algorithm we use the replication material code named ```keyword_algorithm.py```and is provided here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FMJDCD. We also had to update some functions to be compatible with the latest version of Pandas. To display our workflow we go through 1 iteration bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and prepare data for the model\n",
    "data = pd.read_csv(\"data/final_df.csv\", index_col=0)\n",
    "data = data.dropna(subset = [\"final_text\"]).reset_index(drop = True)\n",
    "data[\"index_col\"] = data.index\n",
    "data.to_csv(\"data/query_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download model from https://fasttext.cc/docs/en/english-vectors.html\n",
    "model_dir = \"data/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    \n",
    "    def __init__(self, emb_model):\n",
    "        \n",
    "        self.query = Keywords()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        #Load the data. Change path if necessary\n",
    "        path = 'data/query_df.csv'\n",
    "        self.query.LoadDataset(path, text_colname='final_text', \n",
    "                    date_colname=\"created_at\", id_colname=\"index_col\")\n",
    "        #Pre-trained word embeddings model \n",
    "        self.we = emb_model\n",
    "            \n",
    "    def get_keywords(self, its = 2, top_n = 10, refkeys = [], tarkeys = [], algorithms = ['nbayes', 'logit'], \n",
    "                     date_start = \"2019-06-01\", date_end = \"2020-05-30\"):\n",
    "        \"\"\"\n",
    "        Loops over King. et. al algorithm to extract relevant keywords used for building a \n",
    "        boolean query to subset relevant Tweets in the dataset.\n",
    "        ---------\n",
    "        arguments:\n",
    "            - its: Iterations to run the algorithm\n",
    "            - top_n: integer of how many of the most predictive keywords to extract in each iteration\n",
    "            - refkeys: list of initial keywords used to create reference set of tweets\n",
    "            - tarkeys: list of initial keywords used to limit the search set\n",
    "            - algorithms: list of classifiers to run for extracting keywords \n",
    "            - date_start: y/m/d of start date for relevant tweets\n",
    "            - date_end: y/m/d of end date for relevant tweets\n",
    "        -----------    \n",
    "        returns:\n",
    "            - dictionary of accepted, rejected and nontarget keywords\n",
    "            - the trained query model object\n",
    "        \"\"\"\n",
    "        \n",
    "        accepted_keywords = []\n",
    "        rejected_keywords = []\n",
    "        nontarget_keywords = []\n",
    "        \n",
    "        #Begin loop for mining search set\n",
    "        for it in range(its):\n",
    "            print(\"-\"*66)\n",
    "            print(f\"STARTING ITERATION: {it}!\")\n",
    "            if it == 0:\n",
    "                print(f\"INITIAL REFERENCE KEYS: {refkeys} \\n INITIAL TARGET KEYS: {tarkeys}\")\n",
    "            print(\"-\"*66)\n",
    "            \n",
    "            #Build reference set of tweets\n",
    "            self.query.ReferenceSet(any_words=refkeys, date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            #Use accepted keys as search keys if not the first iteration\n",
    "            if it > 0:\n",
    "                self.query.SearchSet(any_words = accepted_keywords, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            else:\n",
    "                self.query.SearchSet(any_words = tarkeys, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            \n",
    "            #Run King algorithm to find keywords.\n",
    "            self.query.ProcessData(stem = False, keep_twitter_symbols=False,\n",
    "                                   remove_wordlist=refkeys)\n",
    "            self.query.ReferenceKeywords()\n",
    "            self.query.ClassifyDocs(min_df=5, ref_trainprop=1, algorithms=algorithms)\n",
    "            self.query.FindTargetSet()\n",
    "            self.query.FindKeywords()\n",
    "            \n",
    "            #Extract target keywords from algorithm results\n",
    "            target_keywords = self.query.target_keywords[:top_n]\n",
    "            #Also get the reference set keywords to loop over\n",
    "            target_keywords += self.query.reference_keywords[:top_n]\n",
    "            #Append unique nontarget keywords to list of nontarget keys\n",
    "            for nonkey in self.query.nontarget_keywords[:100]:\n",
    "                if nonkey not in nontarget_keywords:\n",
    "                    nontarget_keywords.append(nonkey)\n",
    "            \n",
    "            #Loop over each relevant keyword from reference and found target keywords\n",
    "            for keyword in target_keywords:\n",
    "                #Check if keyword has already been rejected or accepted\n",
    "                if keyword in accepted_keywords or keyword in rejected_keywords:\n",
    "                    continue\n",
    "                else:\n",
    "                    inp = input(f\"Keep {keyword.upper()} yes or no?\")\n",
    "                    if inp == \"y\":\n",
    "                        accepted_keywords.append(keyword)\n",
    "                        #get similar keywords through most similar pretrained embeddings\n",
    "                        inp2 = input(f\"Look at {keyword.upper()}'s most similar word embeddings, yes or no?\")\n",
    "                        if inp2 == \"y\":\n",
    "                            #Look if keyword exist in embedding model dictionary\n",
    "                            try:\n",
    "                                #Get the stemmed embedding \n",
    "                                embeddings = [self.stemmer.stem(emb[0]) for emb in self.we.most_similar(keyword)]\n",
    "                                for emb in embeddings:\n",
    "                                    #Look if embedding already exist in embedding model dictionary\n",
    "                                    if emb.lower() in accepted_keywords or emb.lower() in rejected_keywords:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        inp3 = input(f\"Keep embedding {emb.upper()} yes or no?\")\n",
    "                                        if inp3 == \"y\":\n",
    "                                            accepted_keywords.append(emb)\n",
    "                                        elif inp3 ==\"n\":\n",
    "                                            rejected_keywords.append(emb)\n",
    "                            except:\n",
    "                                print(f\"{keyword.upper()} embedding not present in Model!\")\n",
    "                                pass\n",
    "                        elif inp2 == \"n\":\n",
    "                            pass\n",
    "                    elif inp == \"n\":\n",
    "                        rejected_keywords.append(keyword)\n",
    "                        \n",
    "            #Add custom keyword(s) in the end of the loop. Either as list or single keyword\n",
    "            inp4 = input(f\"Do you wish to add any further keywords? If yes, Type keyword: \")\n",
    "            if inp4:\n",
    "                if isinstance(inp4, list):\n",
    "                    [accepted_keywords.append(key) for key in inp4]\n",
    "                else:\n",
    "                     accepted_keywords.append(inp4)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            print(\"-\"*66)\n",
    "            print(\" \"*20, f\"CURRENT KEYWORDS AFTER ITTERATION {it}\")\n",
    "            print(\"-\"*66)\n",
    "            print(f\"ACCEPTED: \\n {accepted_keywords}\")\n",
    "            print(f\"REJECTED: \\n {rejected_keywords}\")\n",
    "            \n",
    "        fitted_model = self.query\n",
    "        keywords = {\"accepted_keys\":accepted_keywords, \"rejected_keys\":rejected_keywords,\"nontarget_keys\":nontarget_keywords}\n",
    "        \n",
    "        return keywords, fitted_model\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now demostrating the workflow for one iteration. Note that for each iteration the search set is normally expanded by the accepted keywords. We do this untill we see no significant increase in the amount of documents and we are not getting new keywords to either accept or reject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword object initialized.\n",
      "Loaded corpus of size 147440 in 4.02 seconds.\n",
      "------------------------------------------------------------------\n",
      "STARTING ITERATION: 0!\n",
      "INITIAL REFERENCE KEYS: ['bushfir|bushfir_crisi|bushfir_affect|firefight'] \n",
      " INITIAL TARGET KEYS: ['fire', 'disast', 'recov', 'emerg', 'wildlif', 'nsw']\n",
      "------------------------------------------------------------------\n",
      "Loaded reference set of size 1991 in 3.23 seconds.\n",
      "Loaded search set of size 4816 in 7.26 seconds.\n",
      "Time to process corpus: 2.3 seconds\n",
      "\n",
      "4199 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 6807 by 2595 with 92134 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.19 seconds\n",
      "\n",
      "Ref training size: 1991; Search training size: 1589; Training size: 3580; Test size: 4816\n",
      "\n",
      "Time for Naive Bayes: 0.0 seconds\n",
      "Time for Logit: 0.18 seconds\n",
      "1613 documents in target set\n",
      "3203 documents in non-target set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matiasp/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 target set keywords found\n",
      "709 non-target set keywords found\n",
      "Keep FIRE yes or no?y\n",
      "Look at FIRE's most similar word embeddings, yes or no?y\n",
      "Keep embedding FLAME yes or no?y\n",
      "Keep embedding THREE-ALARM yes or no?y\n",
      "Keep embedding TWO-ALARM yes or no?y\n",
      "Keep embedding FOUR-ALARM yes or no?y\n",
      "Keep embedding BLAZE yes or no?y\n",
      "Keep embedding FIVE-ALARM yes or no?y\n",
      "Keep embedding FIRE. yes or no?n\n",
      "Keep embedding CONFLAGR yes or no?y\n",
      "Keep AFFECT yes or no?n\n",
      "Keep RECOVERI yes or no?y\n",
      "Look at RECOVERI's most similar word embeddings, yes or no?y\n",
      "RECOVERI embedding not present in Model!\n",
      "Keep SUPPORT yes or no?y\n",
      "Look at SUPPORT's most similar word embeddings, yes or no?y\n",
      "Keep embedding SUPPPORT yes or no?n\n",
      "Keep embedding OPPOS yes or no?n\n",
      "Keep embedding BACK yes or no?n\n",
      "Keep embedding SUPORT yes or no?n\n",
      "Keep embedding SUPPRT yes or no?n\n",
      "Keep embedding HELP yes or no?y\n",
      "Keep DEVAST yes or no?y\n",
      "Look at DEVAST's most similar word embeddings, yes or no?n\n",
      "Keep SERVIC yes or no?y\n",
      "Look at SERVIC's most similar word embeddings, yes or no?y\n",
      "Keep embedding PBX yes or no?n\n",
      "Keep embedding SPECIALISTS. yes or no?n\n",
      "Keep embedding PROVIDER. yes or no?n\n",
      "Keep embedding UNSPECIFIED. yes or no?n\n",
      "Keep embedding COMPETITIVE. yes or no?n\n",
      "Keep embedding CONSULTANTS. yes or no?n\n",
      "Keep embedding CBINDUSTRI yes or no?n\n",
      "Keep embedding CDF. yes or no?n\n",
      "Keep embedding PROVI yes or no?n\n",
      "Keep embedding OLW. yes or no?n\n",
      "Keep SUMMER yes or no?y\n",
      "Look at SUMMER's most similar word embeddings, yes or no?n\n",
      "Keep THANK yes or no?n\n",
      "Keep AREA yes or no?n\n",
      "Keep COMMUN yes or no?n\n",
      "Keep TODAY yes or no?n\n",
      "Keep GOVERN yes or no?n\n",
      "Keep NEED yes or no?n\n",
      "Keep AUSTRALIA yes or no?n\n",
      "Do you wish to add any further keywords? If yes, Type keyword: koala\n",
      "------------------------------------------------------------------\n",
      "                     CURRENT KEYWORDS AFTER ITTERATION 0\n",
      "------------------------------------------------------------------\n",
      "ACCEPTED: \n",
      " ['fire', 'flame', 'three-alarm', 'two-alarm', 'four-alarm', 'blaze', 'five-alarm', 'conflagr', 'recoveri', 'support', 'help', 'devast', 'servic', 'summer', 'koala']\n",
      "REJECTED: \n",
      " ['fire.', 'affect', 'suppport', 'oppos', 'back', 'suport', 'supprt', 'pbx', 'specialists.', 'provider.', 'unspecified.', 'competitive.', 'consultants.', 'cbindustri', 'cdf.', 'provi', 'olw.', 'thank', 'area', 'commun', 'today', 'govern', 'need', 'australia']\n"
     ]
    }
   ],
   "source": [
    "#Initiate the query builder object\n",
    "query = QueryBuilder(fasttext)\n",
    "#Run the workflow using initial keywords found by our topic models to demarcate reference and search set\n",
    "keywords, model = query.get_keywords(its = 1, top_n=10, \n",
    "                                     refkeys=[\"bushfir|bushfir_crisi|bushfir_affect|firefight\"], \n",
    "                                     tarkeys=[\"fire\", \"disast\", \"recov\", \"emerg\",\"wildlif\", \"nsw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Bushfire Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on keywords found through the process above and our nethnography we define the subset of bushfire tweets bellow. Note that this is the final query which has been refined based on random sampling to and qualitative coding to evaluate accuracy as outlined in section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define keywords for subsetting\n",
    "positive_query = \"firey|bushfir|bushfir_crisi|bushfir_affect|firefight|fire|blaze|/^nsw$/|firefighters|firemen|conflagration|/^ash$/|smoke|aerial|opbushfireassist|burnings|burns|burnt|burned|burn|burning|nswfires|blacksummer|habitat|flames|two-alarm|three-alarm|four-alarm|five-alarm|blaze|fireman|habitats|wild-life\"\n",
    "negative_query = r\"covid|coron|covid|pandemic|epidemic|flu|rona|vaccin|virus\"\n",
    "positive_query = '|'.join(set([PorterStemmer().stem(w) for w in positive_query.split(\"|\")]))\n",
    "negative_query = '|'.join(set([PorterStemmer().stem(w) for w in negative_query.split(\"|\")]))\n",
    "\n",
    "#create subset\n",
    "subset = data.loc[(data[\"final_text\"].str.contains(positive_query))\n",
    "                 &(~data[\"final_text\"].str.contains(negative_query)) \n",
    "                 &(data[\"created_at\"] >= \"2019-06-01\") \n",
    "                 &(data[\"created_at\"] < \"2020-06-01\")].reset_index(drop = True)\n",
    "\n",
    "\n",
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_name</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>name</th>\n",
       "      <th>party</th>\n",
       "      <th>legislative_period</th>\n",
       "      <th>stems</th>\n",
       "      <th>stems_bigrams</th>\n",
       "      <th>final_text</th>\n",
       "      <th>index_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlanTudgeMP</td>\n",
       "      <td>185932331</td>\n",
       "      <td>1252388157321957389</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>Great front page @GippslandTimes a $32m Prince...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['GippslandTimes']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://pbs.twimg.com/media/EWFgY07U8AE-fVE.jpg</td>\n",
       "      <td>Alan Tudge</td>\n",
       "      <td>Liberal Party of Australia</td>\n",
       "      <td>46</td>\n",
       "      <td>['great', 'front', 'page', 'princ', 'highway',...</td>\n",
       "      <td>['great_front', 'front_page', 'page_princ', 'p...</td>\n",
       "      <td>great front page princ highway infra packag re...</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlanTudgeMP</td>\n",
       "      <td>185932331</td>\n",
       "      <td>1235758434697367552</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>The more boots on the ground the better. It’s ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['bushfire']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://pbs.twimg.com/media/ESZLvmyUEAA-1fM.jpg</td>\n",
       "      <td>Alan Tudge</td>\n",
       "      <td>Liberal Party of Australia</td>\n",
       "      <td>46</td>\n",
       "      <td>['boot', 'ground', 'better', 'great', 'see', '...</td>\n",
       "      <td>['boot_ground', 'ground_better', 'better_great...</td>\n",
       "      <td>boot ground better great see backpack make rec...</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlanTudgeMP</td>\n",
       "      <td>185932331</td>\n",
       "      <td>1229956034376028161</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>In Picton today for a $40 million liveability ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WesternSydneyCityDeal']</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://minister.infrastructure.gov.au/tudge/me...</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/122995...</td>\n",
       "      <td>Alan Tudge</td>\n",
       "      <td>Liberal Party of Australia</td>\n",
       "      <td>46</td>\n",
       "      <td>['picton', 'today', 'million', 'liveabl', 'boo...</td>\n",
       "      <td>['picton_today', 'today_million', 'million_liv...</td>\n",
       "      <td>picton today million liveabl boost western syd...</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlanTudgeMP</td>\n",
       "      <td>185932331</td>\n",
       "      <td>1229174711323525125</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>We’re making it easier for backpackers and oth...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://minister.homeaffairs.gov.au/davidcolem...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alan Tudge</td>\n",
       "      <td>Liberal Party of Australia</td>\n",
       "      <td>46</td>\n",
       "      <td>['make', 'easier', 'backpack', 'other', 'work'...</td>\n",
       "      <td>['make_easier', 'easier_backpack', 'backpack_o...</td>\n",
       "      <td>make easier backpack other work holiday visa h...</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlanTudgeMP</td>\n",
       "      <td>185932331</td>\n",
       "      <td>1220917604417556480</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>In the wake of the tragic bushfires, we have s...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://pbs.twimg.com/ext_tw_video_thumb/122090...</td>\n",
       "      <td>Alan Tudge</td>\n",
       "      <td>Liberal Party of Australia</td>\n",
       "      <td>46</td>\n",
       "      <td>['wake', 'tragic', 'bushfir', 'seen', 'incred'...</td>\n",
       "      <td>['wake_tragic', 'tragic_bushfir', 'bushfir_see...</td>\n",
       "      <td>wake tragic bushfir seen incred sens uniti acr...</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>AdamBandt</td>\n",
       "      <td>25960714</td>\n",
       "      <td>1159388987628244992</td>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>This is an SOS from the world’s scientists abo...</td>\n",
       "      <td>239.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Greens']</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://www.abc.net.au/news/science/2019-08-08/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam Bandt</td>\n",
       "      <td>Australian Greens</td>\n",
       "      <td>46</td>\n",
       "      <td>['so', 'world', 'scientist', 'climat', 'emerg'...</td>\n",
       "      <td>['so_world', 'world_scientist', 'scientist_cli...</td>\n",
       "      <td>so world scientist climat emerg aust middl wor...</td>\n",
       "      <td>165995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>AdamBandt</td>\n",
       "      <td>25960714</td>\n",
       "      <td>1159036508621303808</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>RT @climatecouncil: Huge fires burning right n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>climatecouncil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['climatecouncil']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam Bandt</td>\n",
       "      <td>Australian Greens</td>\n",
       "      <td>46</td>\n",
       "      <td>['huge', 'fire', 'burn', 'right', 'arctic', 'r...</td>\n",
       "      <td>['huge_fire', 'fire_burn', 'burn_right', 'righ...</td>\n",
       "      <td>huge fire burn right arctic releas much co atm...</td>\n",
       "      <td>165996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>AdamBandt</td>\n",
       "      <td>25960714</td>\n",
       "      <td>1156847179035631616</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>RT @climatecouncil: Arctic fires released the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>climatecouncil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['climatecouncil']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam Bandt</td>\n",
       "      <td>Australian Greens</td>\n",
       "      <td>46</td>\n",
       "      <td>['arctic', 'fire', 'releas', 'equival', 'swede...</td>\n",
       "      <td>['arctic_fire', 'fire_releas', 'releas_equival...</td>\n",
       "      <td>arctic fire releas equival sweden total annual...</td>\n",
       "      <td>166015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>AdamBandt</td>\n",
       "      <td>25960714</td>\n",
       "      <td>1153806634797322241</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>RT @AOC: The climate crisis will not wait for ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18311.0</td>\n",
       "      <td>AOC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>['AOC']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam Bandt</td>\n",
       "      <td>Australian Greens</td>\n",
       "      <td>46</td>\n",
       "      <td>['climat', 'crisi', 'wait', 'comfort', 'politi...</td>\n",
       "      <td>['climat_crisi', 'crisi_wait', 'wait_comfort',...</td>\n",
       "      <td>climat crisi wait comfort politician consequ b...</td>\n",
       "      <td>166056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>RealBobKatter</td>\n",
       "      <td>310726037</td>\n",
       "      <td>1222261925985210368</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>Thanks for having me this morning @BreakfastNe...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sportrorts', 'fireandthefloods', 'australian...</td>\n",
       "      <td>['BreakfastNews']</td>\n",
       "      <td>https://twitter.com/BreakfastNews/status/12222...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bob Katter</td>\n",
       "      <td>Katter's Australian Party</td>\n",
       "      <td>46</td>\n",
       "      <td>['thank', 'morn', 'import', 'issu', 'discuss',...</td>\n",
       "      <td>['thank_morn', 'morn_import', 'import_issu', '...</td>\n",
       "      <td>thank morn import issu discuss #sportrort #fir...</td>\n",
       "      <td>167130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2976 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        screen_name    user_id             tweet_id  created_at  \\\n",
       "0       AlanTudgeMP  185932331  1252388157321957389  2020-04-21   \n",
       "1       AlanTudgeMP  185932331  1235758434697367552  2020-03-06   \n",
       "2       AlanTudgeMP  185932331  1229956034376028161  2020-02-19   \n",
       "3       AlanTudgeMP  185932331  1229174711323525125  2020-02-16   \n",
       "4       AlanTudgeMP  185932331  1220917604417556480  2020-01-25   \n",
       "...             ...        ...                  ...         ...   \n",
       "2971      AdamBandt   25960714  1159388987628244992  2019-08-08   \n",
       "2972      AdamBandt   25960714  1159036508621303808  2019-08-07   \n",
       "2973      AdamBandt   25960714  1156847179035631616  2019-08-01   \n",
       "2974      AdamBandt   25960714  1153806634797322241  2019-07-23   \n",
       "2975  RealBobKatter  310726037  1222261925985210368  2020-01-28   \n",
       "\n",
       "                                              full_text  favorite_count  \\\n",
       "0     Great front page @GippslandTimes a $32m Prince...             3.0   \n",
       "1     The more boots on the ground the better. It’s ...            11.0   \n",
       "2     In Picton today for a $40 million liveability ...             8.0   \n",
       "3     We’re making it easier for backpackers and oth...            12.0   \n",
       "4     In the wake of the tragic bushfires, we have s...            77.0   \n",
       "...                                                 ...             ...   \n",
       "2971  This is an SOS from the world’s scientists abo...           239.0   \n",
       "2972  RT @climatecouncil: Huge fires burning right n...             0.0   \n",
       "2973  RT @climatecouncil: Arctic fires released the ...             0.0   \n",
       "2974  RT @AOC: The climate crisis will not wait for ...             0.0   \n",
       "2975  Thanks for having me this morning @BreakfastNe...            44.0   \n",
       "\n",
       "      retweet_count    retweet_name in_reply_to_screen_name  \\\n",
       "0               1.0             NaN                     NaN   \n",
       "1               3.0             NaN                     NaN   \n",
       "2               1.0             NaN                     NaN   \n",
       "3               2.0             NaN                     NaN   \n",
       "4              18.0             NaN                     NaN   \n",
       "...             ...             ...                     ...   \n",
       "2971          120.0             NaN                     NaN   \n",
       "2972          101.0  climatecouncil                     NaN   \n",
       "2973           81.0  climatecouncil                     NaN   \n",
       "2974        18311.0             AOC                     NaN   \n",
       "2975           10.0             NaN                     NaN   \n",
       "\n",
       "                                               hashtags       user_mentions  \\\n",
       "0                                                    []  ['GippslandTimes']   \n",
       "1                                          ['bushfire']                  []   \n",
       "2                             ['WesternSydneyCityDeal']                  []   \n",
       "3                                                    []                  []   \n",
       "4                                                    []                  []   \n",
       "...                                                 ...                 ...   \n",
       "2971                                         ['Greens']                  []   \n",
       "2972                                                 []  ['climatecouncil']   \n",
       "2973                                                 []  ['climatecouncil']   \n",
       "2974                                                 []             ['AOC']   \n",
       "2975  ['sportrorts', 'fireandthefloods', 'australian...   ['BreakfastNews']   \n",
       "\n",
       "                                                    url  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2     http://minister.infrastructure.gov.au/tudge/me...   \n",
       "3     https://minister.homeaffairs.gov.au/davidcolem...   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "2971  http://www.abc.net.au/news/science/2019-08-08/...   \n",
       "2972                                                NaN   \n",
       "2973                                                NaN   \n",
       "2974                                                NaN   \n",
       "2975  https://twitter.com/BreakfastNews/status/12222...   \n",
       "\n",
       "                                              image_url        name  \\\n",
       "0        http://pbs.twimg.com/media/EWFgY07U8AE-fVE.jpg  Alan Tudge   \n",
       "1        http://pbs.twimg.com/media/ESZLvmyUEAA-1fM.jpg  Alan Tudge   \n",
       "2     http://pbs.twimg.com/ext_tw_video_thumb/122995...  Alan Tudge   \n",
       "3                                                   NaN  Alan Tudge   \n",
       "4     http://pbs.twimg.com/ext_tw_video_thumb/122090...  Alan Tudge   \n",
       "...                                                 ...         ...   \n",
       "2971                                                NaN  Adam Bandt   \n",
       "2972                                                NaN  Adam Bandt   \n",
       "2973                                                NaN  Adam Bandt   \n",
       "2974                                                NaN  Adam Bandt   \n",
       "2975                                                NaN  Bob Katter   \n",
       "\n",
       "                           party  legislative_period  \\\n",
       "0     Liberal Party of Australia                  46   \n",
       "1     Liberal Party of Australia                  46   \n",
       "2     Liberal Party of Australia                  46   \n",
       "3     Liberal Party of Australia                  46   \n",
       "4     Liberal Party of Australia                  46   \n",
       "...                          ...                 ...   \n",
       "2971           Australian Greens                  46   \n",
       "2972           Australian Greens                  46   \n",
       "2973           Australian Greens                  46   \n",
       "2974           Australian Greens                  46   \n",
       "2975   Katter's Australian Party                  46   \n",
       "\n",
       "                                                  stems  \\\n",
       "0     ['great', 'front', 'page', 'princ', 'highway',...   \n",
       "1     ['boot', 'ground', 'better', 'great', 'see', '...   \n",
       "2     ['picton', 'today', 'million', 'liveabl', 'boo...   \n",
       "3     ['make', 'easier', 'backpack', 'other', 'work'...   \n",
       "4     ['wake', 'tragic', 'bushfir', 'seen', 'incred'...   \n",
       "...                                                 ...   \n",
       "2971  ['so', 'world', 'scientist', 'climat', 'emerg'...   \n",
       "2972  ['huge', 'fire', 'burn', 'right', 'arctic', 'r...   \n",
       "2973  ['arctic', 'fire', 'releas', 'equival', 'swede...   \n",
       "2974  ['climat', 'crisi', 'wait', 'comfort', 'politi...   \n",
       "2975  ['thank', 'morn', 'import', 'issu', 'discuss',...   \n",
       "\n",
       "                                          stems_bigrams  \\\n",
       "0     ['great_front', 'front_page', 'page_princ', 'p...   \n",
       "1     ['boot_ground', 'ground_better', 'better_great...   \n",
       "2     ['picton_today', 'today_million', 'million_liv...   \n",
       "3     ['make_easier', 'easier_backpack', 'backpack_o...   \n",
       "4     ['wake_tragic', 'tragic_bushfir', 'bushfir_see...   \n",
       "...                                                 ...   \n",
       "2971  ['so_world', 'world_scientist', 'scientist_cli...   \n",
       "2972  ['huge_fire', 'fire_burn', 'burn_right', 'righ...   \n",
       "2973  ['arctic_fire', 'fire_releas', 'releas_equival...   \n",
       "2974  ['climat_crisi', 'crisi_wait', 'wait_comfort',...   \n",
       "2975  ['thank_morn', 'morn_import', 'import_issu', '...   \n",
       "\n",
       "                                             final_text  index_col  \n",
       "0     great front page princ highway infra packag re...        740  \n",
       "1     boot ground better great see backpack make rec...        839  \n",
       "2     picton today million liveabl boost western syd...        849  \n",
       "3     make easier backpack other work holiday visa h...        852  \n",
       "4     wake tragic bushfir seen incred sens uniti acr...        872  \n",
       "...                                                 ...        ...  \n",
       "2971  so world scientist climat emerg aust middl wor...     165995  \n",
       "2972  huge fire burn right arctic releas much co atm...     165996  \n",
       "2973  arctic fire releas equival sweden total annual...     166015  \n",
       "2974  climat crisi wait comfort politician consequ b...     166056  \n",
       "2975  thank morn import issu discuss #sportrort #fir...     167130  \n",
       "\n",
       "[2976 rows x 20 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save bushfire subset\n",
    "subset.to_csv(\"data/bushfire_subset.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd379f7bb8e06f7ec29e5b7f14bfd985640a300b5a41de1996cc2754af32f7b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
