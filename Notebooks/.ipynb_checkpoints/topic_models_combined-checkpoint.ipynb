{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLfOqB4vPEtZ"
   },
   "source": [
    "# **LDA and hSBM topic models**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck09mf5MPOz8"
   },
   "source": [
    "## **Modules**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'topsbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89b5ba57d4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtopsbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTopSBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'topsbm'"
     ]
    }
   ],
   "source": [
    "from topsbm import To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzb881IFEvSc"
   },
   "outputs": [],
   "source": [
    "#Graph tool package\n",
    "from sbmtm import sbmtm\n",
    "import graph_tool.all as gt\n",
    "\n",
    "#Necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from datetime import datetime \n",
    "import logging \n",
    "import sys \n",
    "import pickle \n",
    "import requests\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# remove infrequent words\n",
    "from collections import Counter\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#Importing packages for data visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing packages for LDA\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_BjrC99PTOV"
   },
   "source": [
    "## **Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65tLbA0YFM4X"
   },
   "outputs": [],
   "source": [
    "#Read the twitter data\n",
    "path = \"data/final_df.csv\"\n",
    "df = pd.read_csv(path, index_col = 0)\n",
    "\n",
    "#Subset bushfire period\n",
    "df = df.loc[(df[\"created_at\"] >= \"2019-06-01\") & (df[\"created_at\"] <= \"2020-06-01\")] #?\n",
    "\n",
    "# Stringing stemmed tweets\n",
    "df[\"final_text\"] = df[\"final_text\"].astype(str)\n",
    "\n",
    "#Aggregate data on mp and day\n",
    "df_agg = df.groupby(['name','created_at']).agg({\"final_text\":\" \".join, \"party\" : \"first\", \"created_at\":\"first\"}).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHYkRE9VGmzu"
   },
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#Prepare tweets for topic model - are we removing most common word here? If so\n",
    "#--------------------------------\n",
    "def create_dfm(df):\n",
    "    docs = df['stems_bigrams'].str.split()\n",
    "    titles = df[\"party\"]\n",
    "\n",
    "    cutoff = 5\n",
    "    c = Counter()\n",
    "    for doc in docs:\n",
    "        c.update(Counter(doc))\n",
    "    vocab = c.most_common(10000)\n",
    "    vocab = set([w for w,count in vocab if count>5])\n",
    "\n",
    "    # remove words\n",
    "    docs = [[w for w in doc if w in vocab] for doc in docs]\n",
    "    \n",
    "    return docs, vocab\n",
    "\n",
    "\n",
    "    # For LDA prep we remove words like this:\n",
    "#id2word = Dictionary(df['stems']) \n",
    "\n",
    "#Removing very frequent and infrequent words\n",
    "#id2word.filter_extremes(no_below=10, no_above=.999, keep_n = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df = 8)\n",
    "X = vec.fit_transform(df_agg[\"final_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh7SFsoxPZ49"
   },
   "source": [
    "## **hSBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0L_nIYVJV46"
   },
   "outputs": [],
   "source": [
    "docs, vocab = create_dfm(df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa50q8BLJ-EF"
   },
   "outputs": [],
   "source": [
    "#Note to self: docs is a list of docs, each containing list of words\n",
    "## we create an instance of the sbmtm-class\n",
    "model = sbmtm()\n",
    "\n",
    "## we have to create the word-document network from the corpus\n",
    "model.fit_transform(X)\n",
    "\n",
    "## fit the model\n",
    "gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1g1ik20JV4_"
   },
   "outputs": [],
   "source": [
    "###----------------------------##\n",
    "### Load trained model####\n",
    "###----------------------------##\n",
    "\n",
    "model.save_graph(filename = 'lemmas_graph.xml.gz')\n",
    "#model = sbmtm()\n",
    "#model.load_graph(filename = 'poslemmas_graph.xml.gz')\n",
    "#gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n",
    "#model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sfmxdkRJV5A"
   },
   "outputs": [],
   "source": [
    "#Plot the graph\n",
    "model.plot(nedges=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyIez6xXJV5A",
    "outputId": "d462ed02-3338-4b15-b704-7aa175402e33"
   },
   "outputs": [],
   "source": [
    "model.topics(l = 1, n = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R1Z_TZyJV5C"
   },
   "outputs": [],
   "source": [
    "model.print_topics(l = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL4h5HG5JV5D"
   },
   "outputs": [],
   "source": [
    "len(model.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YvCxh-OJV5D",
    "outputId": "92cad0a3-6e4b-452b-9960-f0d24db92371"
   },
   "outputs": [],
   "source": [
    "#This takes ages, about 6h for me...\n",
    "doc_topic_dist = []\n",
    "for doc_i in tqdm(range(len(model.documents))):\n",
    "    topic_dist = model.topicdist(doc_i, l = 1)\n",
    "    doc_topic_dist.append(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR9gpvpIJV5E"
   },
   "outputs": [],
   "source": [
    "df_agg[\"topic_dist\"] = doc_topic_dist\n",
    "df.to_csv(\"lemmas_topic_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_zVJugWJV5F",
    "outputId": "32717ecd-66d3-466f-ee1e-393aea0a9528"
   },
   "outputs": [],
   "source": [
    "model.topics(l=1,n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfrFLWNyJV5G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_agg[\"bushfire_topic\"] = df_agg[\"topic_dist\"].apply(lambda row: row[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZQvLiGvJV5H"
   },
   "outputs": [],
   "source": [
    "df_week = df_agg.groupby(\"party\").resample('W-Mon', on='created_at').agg({\"party\":\"first\", \"created_at\":\"first\", \"bushfire_topic\":\"mean\"}).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZV7Hq6-JV5I",
    "outputId": "39f7b84a-c906-4ed1-986e-466abf7d44fc"
   },
   "outputs": [],
   "source": [
    "df_week[\"party\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOhuJ4JOJV5K",
    "outputId": "440ffb93-fb52-4959-ac63-c98013a68d11"
   },
   "outputs": [],
   "source": [
    "bliberals = df_week.loc[df_week[\"party\"] == \"Liberal Party of Australia\"]\n",
    "greens = df_week.loc[df_week[\"party\"] == \"Australian Greens\"]\n",
    "labor = df_week.loc[df_week[\"party\"] == \"Australian Labor Party\"]\n",
    "center = df_week.loc[df_week[\"party\"] == \"Centre Alliance\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10), dpi=200)\n",
    "#plt.figure()\n",
    "ax.plot(liberals[\"created_at\"], liberals[\"bushfire_topic\"], 'bo--', label = \"Liberals\")\n",
    "ax.plot(labor[\"created_at\"], labor[\"bushfire_topic\"], 'r^--', label = \"Labor\")\n",
    "ax.plot(greens[\"created_at\"], greens[\"bushfire_topic\"], 'g*--', label = \"Greens\")\n",
    "ax.plot(center[\"created_at\"], center[\"bushfire_topic\"], 'y+--', label = \"Center\")\n",
    "ax.set_ylabel(\"Bushfire Topic Proportion\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(\"topic_prop.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGS3m_41N-0U"
   },
   "source": [
    "# **LDA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz1bRbs6N8oh"
   },
   "outputs": [],
   "source": [
    "#Creating a corpus object \n",
    "corpus = [id2word.doc2bow(doc) for doc in df['stems']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS2VsmJHOIIt"
   },
   "outputs": [],
   "source": [
    "#Instantiating a logger to get information on the progress of the LDA\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Running the LDA with 23 topics \n",
    "lda_model = LdaMulticore(corpus=corpus, num_topics=23, id2word=id2word, passes = 100, iterations = 1000, alpha = \"auto\")\n",
    "\n",
    "#Disabling logging\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbULbvXgON4z"
   },
   "outputs": [],
   "source": [
    "#Creating a dataframe with tokens as columns and topics as rows \n",
    "beta_df = pd.DataFrame(lda_model.get_topics(), columns = list(id2word.token2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trf8NryOOP1o"
   },
   "outputs": [],
   "source": [
    "#Vizualising the 23 topics\n",
    "topics = list(range(0, 22))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, nrows=5, figsize = (16,30)) \n",
    "fig.tight_layout(pad=8.0)\n",
    "    \n",
    "for ax, topic in zip(axes.flatten(), topics): #Iterates through the axes and the topics\n",
    "    \n",
    "    #Creates 20 barplots with x as the beta values and y as the words\n",
    "    sns.barplot(ax = ax, \n",
    "                x = beta_df.iloc[topic].nlargest(10).values, \n",
    "                y = beta_df.iloc[topic].nlargest(10).index, \n",
    "                orient = 'h')\n",
    "    \n",
    "    ax.set(title='Topic {}'.format(topic), xlabel='beta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DZe4A_fOiPA"
   },
   "outputs": [],
   "source": [
    "#Insert the corpus of documents in bag of word format and get a list of (topic, probability) per document. \n",
    "document_topics = list(lda_model.get_document_topics(corpus)) \n",
    "\n",
    "#Creating a list of names for all 23 topics\n",
    "topics = ['topic_{}'.format(t) for t in range(0,22)]\n",
    "\n",
    "#Creating a dataframe of gamma probabilities \n",
    "gamma_probs = pd.DataFrame(np.zeros((len(document_topics),20)), columns = topics)\n",
    "\n",
    "for i, doc in enumerate(document_topics): \n",
    "    for pair in doc: \n",
    "        gamma_probs.loc[i,'topic_{}'.format(pair[0])] = pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vaK_YUlOswm"
   },
   "outputs": [],
   "source": [
    "#Merging with the original dataframe \n",
    "df = pd.concat((df, gamma_probs), axis = 1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hsbm_topic_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
