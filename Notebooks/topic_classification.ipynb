{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the output from the topic model and our immersion journal/manual to subset a training set of tweets about the Australian Bushfires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm #to create a progress bar\n",
    "#Machine learning packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Word embeddings\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "#Gary King et. al key-words\n",
    "from keyword_algorithm import *\n",
    "#Remove unwarranted warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gary Kings semi-automated keyword retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and prepare data\n",
    "data = pd.read_csv(\"data/final_df.csv\", index_col=0)\n",
    "data = data.dropna(subset = [\"lemmas\"]).reset_index(drop = True)\n",
    "data[\"index_col\"] = data.index\n",
    "data.to_csv(\"data/query_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download model from https://fasttext.cc/docs/en/english-vectors.html\n",
    "model_dir = \"data/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    \n",
    "    def __init__(self, emb_model):\n",
    "        \n",
    "        self.query = Keywords()\n",
    "        #Load the data. Change path if necessary\n",
    "        path = 'data/query_df.csv'\n",
    "        self.query.LoadDataset(path, text_colname='lemmas', \n",
    "                    date_colname=\"created_at\", id_colname=\"index_col\")\n",
    "        #Load Gensim word embeddings model \n",
    "        self.we = emb_model\n",
    "            \n",
    "    def get_keywords(self, its = 2, top_n = 10, refkeys = [], tarkeys = [], algorithms = ['nbayes', 'logit']), \n",
    "                     date_start = \"2019-06-01\", date_end = \"2020-05-30\":\n",
    "        \"\"\"\n",
    "        Loops over King. et. al algorithm to extract relevant keywords used for building a \n",
    "        boolean query to subset relevant Tweets in the dataset.\n",
    "        ---------\n",
    "        arguments:\n",
    "            - its: Iterations to run the algorithm\n",
    "            - top_n: integer of how many of the most predictive keywords to extract in each iteration\n",
    "            - refkeys: list of initial keywords used to create reference set of tweets\n",
    "            - tarkeys: list of initial keywords used to limit the search set\n",
    "            - algorithms: list of classifiers to run for extracting keywords \n",
    "            - date_start: y/m/d of start date for relevant tweets\n",
    "            - date_end: y/m/d of end date for relevant tweets\n",
    "        -----------    \n",
    "        returns:\n",
    "            - list of accepted keywords\n",
    "            - list of rejected keywords\n",
    "            - list of nontarget keywords\n",
    "        \"\"\"\n",
    "        \n",
    "        accepted_keywords = []\n",
    "        rejected_keywords = []\n",
    "        nontarget_keywords = []\n",
    "        \n",
    "        #Begin loop for mining search set\n",
    "        for it in range(its):\n",
    "            print(\"-\"*66)\n",
    "            print(f\"STARTING ITERATION: {it}!\")\n",
    "            if it == 0:\n",
    "                print(\"INITIAL REFERENCE KEYS: {refkeys} \\n INITIAL TARGET KEYS: {tarkeys}\")\n",
    "            print(\"-\"*66)\n",
    "            \n",
    "            #Build reference set of tweets\n",
    "            self.query.ReferenceSet(any_words=refkeys, date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            #Use accepted keys as search keys if not the first iteration\n",
    "            if it > 0:\n",
    "                #fit model on searchset and find target\n",
    "                self.query.SearchSet(any_words= accepted_keywords, date_start=date_start, date_end=date_end)\n",
    "            #Use accepted keys to demarcate the search set\n",
    "            else:\n",
    "                self.query.SearchSet(any_words = tarkeys, date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            \n",
    "            #Run King algorithm to find keywords.\n",
    "            self.query.ProcessData(stem = False, keep_twitter_symbols=False,\n",
    "                                   remove_wordlist=refkeys)\n",
    "            self.query.ReferenceKeywords()\n",
    "            self.query.ClassifyDocs(min_df=5, ref_trainprop=1, algorithms=algorithms)\n",
    "            self.query.FindTargetSet()\n",
    "            self.query.FindKeywords()\n",
    "            \n",
    "            #Extract target keywords from algorithm results\n",
    "            target_keywords = self.query.target_keywords[:top_n]\n",
    "            #Also get the reference set keywords to loop over\n",
    "            target_keywords += self.query.reference_keywords[:top_n]\n",
    "            #Append unique nontarget keywords to list of nontarget keys\n",
    "            for nonkey in self.query.nontarget_keywords[:100]:\n",
    "                if nonkey not in nontarget_keywords:\n",
    "                    nontarget_keywords.append(nonkey)\n",
    "            \n",
    "            #Loop over each relevant keyword from reference and found target keywords\n",
    "            for keyword in target_keywords:\n",
    "                #Check if keyword has already been rejected or accepted\n",
    "                if keyword in accepted_keywords or keyword in rejected_keywords:\n",
    "                    continue\n",
    "                else:\n",
    "                    inp = input(f\"Keep {keyword.upper()} yes or no?\")\n",
    "                    if inp == \"yes\":\n",
    "                        accepted_keywords.append(keyword)\n",
    "                        #get similar keywords through most similar pretrained embeddings\n",
    "                        inp2 = input(f\"Look at {keyword.upper()}'s most similar word embeddings, yes or no?\")\n",
    "                        if inp2 == \"yes\":\n",
    "                            #Look if keyword exist in embedding model dictionary\n",
    "                            try:\n",
    "                                embeddings = [emb[0] for emb in self.we.most_similar(keyword)]\n",
    "                                for emb in embeddings:\n",
    "                                    if emb.lower() in accepted_keywords or emb.lower() in rejected_keywords:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        inp3 = input(f\"Keep embedding {emb.upper()} yes or no?\")\n",
    "                                        if inp3 == \"yes\":\n",
    "                                            accepted_keywords.append(emb)\n",
    "                                        elif inp3 ==\"no\":\n",
    "                                            rejected_keywords.append(emb)\n",
    "                            except:\n",
    "                                print(f\"{keyword.upper()} embedding not present in Model!\")\n",
    "                                pass\n",
    "                        elif inp2 == \"no\":\n",
    "                            pass\n",
    "                    elif inp == \"no\":\n",
    "                        rejected_keywords.append(keyword)\n",
    "                        \n",
    "            #Add custom keyword(s) in the end of the loop. Either as list or single keyword\n",
    "            inp4 = input(f\"Do you wish to add any further keywords? If yes, Type keyword: \")\n",
    "            if inp4:\n",
    "                if isinstance(inp4, list):\n",
    "                    [accepted_keywords.append(key) for key in inp4]\n",
    "                else:\n",
    "                     accepted_keywords.append(inp4)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            print(\"-\"*66)\n",
    "            print(\" \"*20, f\"CURRENT KEYWORDS AFTER ITTERATION {it}\")\n",
    "            print(\"-\"*66)\n",
    "            print(f\"ACCEPTED: \\n {accepted_keywords}\")\n",
    "            print(f\"REJECTED: \\n {rejected_keywords}\")\n",
    "            \n",
    "        \n",
    "        keywords = {\"accepted_keys\":accepted_keywords, \"rejected_keys\":rejected_keywords,\"nontarget_keys\":nontarget_keywords}\n",
    "        \n",
    "        return keywords\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword object initialized.\n",
      "Loaded corpus of size 148540 in 3.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "query = QueryBuilder(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = query.get_keywords(2, top_n=10, refkeys=[\"#bushfire\", \"#bushfires\", \"bushfire\", \"bushfires\"], tarkeys = [\"fire\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"|\".join(keywords[\"accepted_keys\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mlogit Model - Average Marginal Effect Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the key-words from Kings model to subset tweets. Next we use this subset to predict the party of the tweet and analyze the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlogitMargins:\n",
    "    \"\"\"\n",
    "    Calculates marginal effect of multinomial logit coefficients with bootstraped confidence interval.\n",
    "    See https://github.com/alicehwu/gendered_language/blob/master/gendered_language_2018.pdf for a \n",
    "    reference on a similar approach but with binary classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        #Define data\n",
    "        self.vect = CountVectorizer(ngram_range=(1,2))\n",
    "        self.X = self.vect.fit_transform(X)\n",
    "        self.y = y\n",
    "        print(\"Fitting model and calculating margins...\")\n",
    "        #Fit model and calculate average marginal effects\n",
    "        self.margins, self.fitted_model = self.avg_margins(self.X, self.y)\n",
    "        \n",
    "\n",
    "    def avg_margins(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates average marginal effect of coefficients in multinomial logit model, where each coefficient is \n",
    "        a word token and is calculated as:\n",
    "        avg_margin_jk = beta_jk * 1/N * sum{P(y_i = J) * 1-P(y_i = J)} for each token k and class j.\n",
    "        See: https://math.stackexchange.com/questions/863258/deriving-marginal-effects-in-multinomial-logit-model\n",
    "        -----------\n",
    "        Returns:\n",
    "            - DataFrame with average margins of shape classes_j * tokens_k\n",
    "            - The fitted mlogit model \n",
    "        \"\"\"\n",
    "        #Fit the model\n",
    "        model = self.fit_model(X, y)\n",
    "        #Get probabilities for each obs i belonging to class j. shape = j * N\n",
    "        probas = model.predict_proba(X)\n",
    "        #Get coefficients. Shape j_classes * k_coefficients\n",
    "        betas = model.coef_\n",
    "\n",
    "        margins = {}\n",
    "        #Loop over each class \n",
    "        for class_j in range(len(model.classes_)):\n",
    "            #Extract corresponging betas. shape 1*k\n",
    "            for beta_jk in betas[class_j]:\n",
    "                \n",
    "                #Calculate avg margins for the jth class and kth beta (token)\n",
    "                margins_jk = beta_jk * 1/probas.shape[0] * (probas[:,class_j] * (1 - probas[:,class_j])).sum()\n",
    "                \n",
    "                #Append avg margins to dictionary\n",
    "                if model.classes_[class_j] not in margins.keys():\n",
    "                    margins[model.classes_[class_j]] = [margins_jk]\n",
    "                else:\n",
    "                    margins[model.classes_[class_j]].append(margins_jk)\n",
    "        \n",
    "        #Extract the token name corresponding to the avg margins   \n",
    "        margins[\"token\"] = self.vect.get_feature_names()\n",
    "        margins_df = pd.DataFrame(margins)\n",
    "        \n",
    "        return margins_df, model\n",
    "    \n",
    "    def fit_model(self, X, y, max_iter = 10000, penalty = \"none\", class_weight = \"balanced\",\n",
    "                  verbose = False, fit_intercept = True, multi_class = \"multinomial\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fits sklearn multinomial logistic model on data. Default penalty set to \"none\" for unbaised estimators.\n",
    "        \"\"\"\n",
    "\n",
    "        mlogit = LogisticRegression(random_state=42, penalty=penalty, solver=\"saga\", \n",
    "                                    max_iter = max_iter, class_weight = class_weight,\n",
    "                                    fit_intercept=fit_intercept, multi_class=multi_class,\n",
    "                                    verbose=verbose).fit(X, y) #   \n",
    "\n",
    "        return mlogit\n",
    "    \n",
    "    def bootstrap_ci(self, alpha = 0.05, n_samples = 500, sample_prop = 0.4):\n",
    "        \"\"\"\n",
    "        Uses bootstraping to calculate confidence interval around average marginal effects estimate.\n",
    "        -------------\n",
    "        Arguments:\n",
    "            - alpha: deterimines confidence level of the interval\n",
    "            - n_samples: amount of bootstrap samples\n",
    "            - sample_prop: bootstramp sample size as proportion of original sample\n",
    "        -------------\n",
    "        Return:\n",
    "            - Pandas dataframe with confidence interval for average marginal effect\n",
    "              of each coefficient.\n",
    "        \"\"\"\n",
    "        statistics = [] #List for bootstrap results\n",
    "        \n",
    "        #Define the bootstrap sample size based on proportion of total\n",
    "        n_size = int(self.X.shape[0] * sample_prop)\n",
    "        print(f\"Number of obs in bootstrap samples {n_size}...\")\n",
    "        start = time.time()\n",
    "        for i in tqdm(range(n_samples)):\n",
    "\n",
    "            #Draw random sample from X and y with replacement\n",
    "            idx = np.random.choice(np.arange(self.X.shape[0]), n_size, replace=True)\n",
    "            X_sample = self.X[idx]\n",
    "            y_sample = self.y[idx]\n",
    "            #Calculate marginal effect for sample\n",
    "            margins = self.avg_margins(X_sample, y_sample)[0]\n",
    "            statistics.append(margins)\n",
    "        \n",
    "        #Join the resulting dataframes of margins\n",
    "        statistics = pd.concat(statistics)\n",
    "        print(f\"Boostraping Done. Calculating {1-alpha}% confidence interval...\")\n",
    "        #From bootstrap results get lower and upper CI limits based on alpha\n",
    "        statistics = statistics.groupby(\"token\").quantile([alpha, 1-alpha]).reset_index()        \n",
    "        print(\"Time to complete: \", time.time() - start)\n",
    "\n",
    "        return statistics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This data is just for testing - not final subset.\n",
    "mlogit_data = data.loc[(data[\"lemmas\"].str.contains(\"bushfire|bushfires|#bushfire|#bushfires|disaster|kaoala|fire|flames|firemen\") == True) & \n",
    "                       (data[\"lemmas\"].str.contains(\"corona|covid\") == False) & \n",
    "                       (data[\"created_at\"] >= \"2019-06-01\") & \n",
    "                       (data[\"created_at\"] <= \"2020-06-01\")]\n",
    "\n",
    "#Reduce classes to predict. Otherwise bad model performance.\n",
    "mlogit_data = mlogit_data.loc[mlogit_data[\"party\"].isin([\"Australian Greens\",\n",
    "                                                         \"Australian Labor Party\", \n",
    "                                                         \"Liberal Party of Australia\",\n",
    "                                                          ])].dropna(subset = [\"lemmas\"]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model and calculating margins...\n"
     ]
    }
   ],
   "source": [
    "X, y = mlogit_data[\"lemmas\"], mlogit_data[\"party\"]\n",
    "mlogit = MlogitMargins(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>0.003002</td>\n",
       "      <td>-0.002033</td>\n",
       "      <td>-0.001520</td>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.002593</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>climateemergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>0.002226</td>\n",
       "      <td>-0.001706</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>coal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7119</th>\n",
       "      <td>0.001402</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>especially case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>0.001402</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>case bushfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13008</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>-0.000751</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>0.001281</td>\n",
       "      <td>-0.000797</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>0.001191</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4111</th>\n",
       "      <td>0.001151</td>\n",
       "      <td>-0.001591</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>climate emergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>0.001125</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16202</th>\n",
       "      <td>0.001089</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>prayer bushfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>0.001089</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>bushfires climateemergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9812</th>\n",
       "      <td>0.001085</td>\n",
       "      <td>-0.001741</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>greens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21468</th>\n",
       "      <td>0.001025</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>-0.000598</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>0.001006</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>catastrophic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Australian Greens  Australian Labor Party  Liberal Party of Australia  \\\n",
       "4095            0.003002               -0.002033                   -0.001520   \n",
       "4155            0.002635               -0.002593                   -0.000964   \n",
       "4222            0.002226               -0.001706                   -0.001036   \n",
       "7119            0.001402               -0.001060                   -0.000659   \n",
       "3532            0.001402               -0.001060                   -0.000659   \n",
       "13008           0.001315               -0.000705                   -0.000751   \n",
       "5449            0.001281               -0.000797                   -0.000681   \n",
       "1850            0.001191               -0.000186                   -0.000888   \n",
       "4111            0.001151               -0.001591                   -0.000211   \n",
       "3530            0.001125               -0.000400                   -0.000736   \n",
       "16202           0.001089               -0.000811                   -0.000517   \n",
       "2973            0.001089               -0.000811                   -0.000517   \n",
       "9812            0.001085               -0.001741                   -0.000088   \n",
       "21468           0.001025               -0.000521                   -0.000598   \n",
       "3557            0.001006               -0.000732                   -0.000486   \n",
       "\n",
       "                            token  \n",
       "4095                      climate  \n",
       "4155             climateemergency  \n",
       "4222                         coal  \n",
       "7119              especially case  \n",
       "3532                case bushfire  \n",
       "13008                        many  \n",
       "5449                          cut  \n",
       "1850                          bad  \n",
       "4111            climate emergency  \n",
       "3530                         case  \n",
       "16202            prayer bushfires  \n",
       "2973   bushfires climateemergency  \n",
       "9812                       greens  \n",
       "21468                        town  \n",
       "3557                 catastrophic  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlogit.margins.sort_values(\"Australian Greens\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obs in bootstrap samples 880...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:10<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boostraping Done. Calculating 0.95% confidence interval...\n",
      "Time to calculate:  0.403839111328125\n"
     ]
    }
   ],
   "source": [
    "mlogit_ci = mlogit.bootstrap_ci(alpha=0.05, n_samples=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>level_1</th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.001071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>-0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon morrison</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon morrison</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abate</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abate</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abate community</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abate community</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abc</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.000932</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abc</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token  level_1  Australian Greens  Australian Labor Party  \\\n",
       "0           abandon     0.05          -0.000637                0.000031   \n",
       "1           abandon     0.95          -0.000007                0.001977   \n",
       "2  abandon morrison     0.05          -0.000022                0.000000   \n",
       "3  abandon morrison     0.95           0.000000                0.000866   \n",
       "4             abate     0.05          -0.000114                0.000000   \n",
       "5             abate     0.95           0.000000                0.000387   \n",
       "6   abate community     0.05          -0.000114                0.000000   \n",
       "7   abate community     0.95           0.000000                0.000387   \n",
       "8               abc     0.05          -0.000932               -0.000534   \n",
       "9               abc     0.95          -0.000061                0.002493   \n",
       "\n",
       "   Liberal Party of Australia  \n",
       "0                   -0.001071  \n",
       "1                   -0.000017  \n",
       "2                   -0.000259  \n",
       "3                    0.000000  \n",
       "4                   -0.000150  \n",
       "5                    0.000000  \n",
       "6                   -0.000150  \n",
       "7                    0.000000  \n",
       "8                   -0.001135  \n",
       "9                    0.000369  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlogit_ci.head(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd379f7bb8e06f7ec29e5b7f14bfd985640a300b5a41de1996cc2754af32f7b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
