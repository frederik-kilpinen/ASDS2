{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the output from the topic model and our immersion journal/manual to subset a training set of tweets about the Australian Bushfires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm #to create a progress bar\n",
    "#Machine learning packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Word embeddings\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "#Gary King et. al key-words\n",
    "from keyword_algorithm import *\n",
    "#Remove unwarranted warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gary Kings semi-automated keyword retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and prepare data\n",
    "data = pd.read_csv(\"data/final_df.csv\", index_col=0)\n",
    "data = data.dropna(subset = [\"lemmas\"]).reset_index(drop = True)\n",
    "data[\"index_col\"] = data.index\n",
    "#data.to_csv(\"data/query_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download model from https://fasttext.cc/docs/en/english-vectors.html\n",
    "model_dir = \"data/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    \n",
    "    def __init__(self, emb_model):\n",
    "        \n",
    "        self.query = Keywords()\n",
    "        #Load the data. Change path if necessary\n",
    "        path = 'data/query_df.csv'\n",
    "        self.query.LoadDataset(path, text_colname='lemmas', \n",
    "                    date_colname=\"created_at\", id_colname=\"index_col\")\n",
    "        #Load Gensim word embeddings model \n",
    "        self.we = emb_model\n",
    "            \n",
    "    def get_keywords(self, its = 2, top_n = 10, refkeys = [], tarkeys = [], algorithms = ['nbayes', 'logit'], \n",
    "                     date_start = \"2019-06-01\", date_end = \"2020-05-30\"):\n",
    "        \"\"\"\n",
    "        Loops over King. et. al algorithm to extract relevant keywords used for building a \n",
    "        boolean query to subset relevant Tweets in the dataset.\n",
    "        ---------\n",
    "        arguments:\n",
    "            - its: Iterations to run the algorithm\n",
    "            - top_n: integer of how many of the most predictive keywords to extract in each iteration\n",
    "            - refkeys: list of initial keywords used to create reference set of tweets\n",
    "            - tarkeys: list of initial keywords used to limit the search set\n",
    "            - algorithms: list of classifiers to run for extracting keywords \n",
    "            - date_start: y/m/d of start date for relevant tweets\n",
    "            - date_end: y/m/d of end date for relevant tweets\n",
    "        -----------    \n",
    "        returns:\n",
    "            - dictionary of accepted, rejected and nontarget keywords\n",
    "        \"\"\"\n",
    "        \n",
    "        accepted_keywords = []\n",
    "        rejected_keywords = []\n",
    "        nontarget_keywords = []\n",
    "        \n",
    "        #Begin loop for mining search set\n",
    "        for it in range(its):\n",
    "            print(\"-\"*66)\n",
    "            print(f\"STARTING ITERATION: {it}!\")\n",
    "            if it == 0:\n",
    "                print(\"INITIAL REFERENCE KEYS: {refkeys} \\n INITIAL TARGET KEYS: {tarkeys}\")\n",
    "            print(\"-\"*66)\n",
    "            \n",
    "            #Build reference set of tweets\n",
    "            self.query.ReferenceSet(any_words=refkeys, date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            #Use accepted keys as search keys if not the first iteration\n",
    "            if it > 0:\n",
    "                self.query.SearchSet(any_words = accepted_keywords, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            else:\n",
    "                self.query.SearchSet(any_words = tarkeys, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            \n",
    "            #Run King algorithm to find keywords.\n",
    "            self.query.ProcessData(stem = False, keep_twitter_symbols=False,\n",
    "                                   remove_wordlist=refkeys)\n",
    "            self.query.ReferenceKeywords()\n",
    "            self.query.ClassifyDocs(min_df=5, ref_trainprop=1, algorithms=algorithms)\n",
    "            self.query.FindTargetSet()\n",
    "            self.query.FindKeywords()\n",
    "            \n",
    "            #Extract target keywords from algorithm results\n",
    "            target_keywords = self.query.target_keywords[:top_n]\n",
    "            #Also get the reference set keywords to loop over\n",
    "            target_keywords += self.query.reference_keywords[:top_n]\n",
    "            #Append unique nontarget keywords to list of nontarget keys\n",
    "            for nonkey in self.query.nontarget_keywords[:100]:\n",
    "                if nonkey not in nontarget_keywords:\n",
    "                    nontarget_keywords.append(nonkey)\n",
    "            \n",
    "            #Loop over each relevant keyword from reference and found target keywords\n",
    "            for keyword in target_keywords:\n",
    "                #Check if keyword has already been rejected or accepted\n",
    "                if keyword in accepted_keywords or keyword in rejected_keywords:\n",
    "                    continue\n",
    "                else:\n",
    "                    inp = input(f\"Keep {keyword.upper()} yes or no?\")\n",
    "                    if inp == \"yes\":\n",
    "                        accepted_keywords.append(keyword)\n",
    "                        #get similar keywords through most similar pretrained embeddings\n",
    "                        inp2 = input(f\"Look at {keyword.upper()}'s most similar word embeddings, yes or no?\")\n",
    "                        if inp2 == \"yes\":\n",
    "                            #Look if keyword exist in embedding model dictionary\n",
    "                            try:\n",
    "                                embeddings = [emb[0] for emb in self.we.most_similar(keyword)]\n",
    "                                for emb in embeddings:\n",
    "                                    if emb.lower() in accepted_keywords or emb.lower() in rejected_keywords:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        inp3 = input(f\"Keep embedding {emb.upper()} yes or no?\")\n",
    "                                        if inp3 == \"yes\":\n",
    "                                            accepted_keywords.append(emb)\n",
    "                                        elif inp3 ==\"no\":\n",
    "                                            rejected_keywords.append(emb)\n",
    "                            except:\n",
    "                                print(f\"{keyword.upper()} embedding not present in Model!\")\n",
    "                                pass\n",
    "                        elif inp2 == \"no\":\n",
    "                            pass\n",
    "                    elif inp == \"no\":\n",
    "                        rejected_keywords.append(keyword)\n",
    "                        \n",
    "            #Add custom keyword(s) in the end of the loop. Either as list or single keyword\n",
    "            inp4 = input(f\"Do you wish to add any further keywords? If yes, Type keyword: \")\n",
    "            if inp4:\n",
    "                if isinstance(inp4, list):\n",
    "                    [accepted_keywords.append(key) for key in inp4]\n",
    "                else:\n",
    "                     accepted_keywords.append(inp4)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            print(\"-\"*66)\n",
    "            print(\" \"*20, f\"CURRENT KEYWORDS AFTER ITTERATION {it}\")\n",
    "            print(\"-\"*66)\n",
    "            print(f\"ACCEPTED: \\n {accepted_keywords}\")\n",
    "            print(f\"REJECTED: \\n {rejected_keywords}\")\n",
    "            \n",
    "        \n",
    "        keywords = {\"accepted_keys\":accepted_keywords, \"rejected_keys\":rejected_keywords,\"nontarget_keys\":nontarget_keywords}\n",
    "        \n",
    "        return keywords\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword object initialized.\n",
      "Loaded corpus of size 148540 in 2.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "query = QueryBuilder(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "STARTING ITERATION: 0!\n",
      "INITIAL REFERENCE KEYS: {refkeys} \n",
      " INITIAL TARGET KEYS: {tarkeys}\n",
      "------------------------------------------------------------------\n",
      "Loaded reference set of size 1682 in 2.09 seconds.\n",
      "Loaded search set of size 1368 in 0.52 seconds.\n",
      "Time to process corpus: 0.49 seconds\n",
      "\n",
      "4159 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 3050 by 1495 with 37198 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.05 seconds\n",
      "\n",
      "Ref training size: 1682; Search training size: 451; Training size: 2133; Test size: 1368\n",
      "\n",
      "Time for Naive Bayes: 0.01 seconds\n",
      "Time for Logit: 0.04 seconds\n",
      "473 documents in target set\n",
      "895 documents in non-target set\n",
      "253 target set keywords found\n",
      "185 non-target set keywords found\n",
      "Keep SUPPORT yes or no? yes\n",
      "Look at SUPPORT's most similar word embeddings, yes or no? yes\n",
      "Keep embedding SUPPORTING yes or no? yes\n",
      "Keep embedding SUPPPORT yes or no? yes\n",
      "Keep embedding SUPPORTED yes or no? yes\n",
      "Keep embedding OPPOSE yes or no? no\n",
      "Keep embedding BACKING yes or no? no\n",
      "Keep embedding SUPORT yes or no? no\n",
      "Keep embedding SUPPORTS yes or no? no\n",
      "Keep embedding SUPPRT yes or no? no\n",
      "Keep embedding HELP yes or no? no\n",
      "Keep GOVERNMENT yes or no? no\n",
      "Keep RECOVERY yes or no? yes\n",
      "Look at RECOVERY's most similar word embeddings, yes or no? no\n",
      "Keep AFFECT yes or no? yes\n",
      "Look at AFFECT's most similar word embeddings, yes or no? no\n",
      "Keep MORRISON yes or no? yes\n",
      "Look at MORRISON's most similar word embeddings, yes or no? no\n",
      "Keep CHIEF yes or no? no\n",
      "Keep REBUILD yes or no? no\n",
      "Keep BUSINESS yes or no? no\n",
      "Keep COMMUNITY yes or no? no\n",
      "Keep ANNOUNCE yes or no? no\n",
      "Keep TODAY yes or no? no\n",
      "Keep NEED yes or no? no\n",
      "Keep AUSPOL yes or no? no\n",
      "Keep PEOPLE yes or no? no\n",
      "Do you wish to add any further keywords? If yes, Type keyword:  koala\n",
      "------------------------------------------------------------------\n",
      "                     CURRENT KEYWORDS AFTER ITTERATION 0\n",
      "------------------------------------------------------------------\n",
      "ACCEPTED: \n",
      " ['support', 'supporting', 'suppport', 'supported', 'recovery', 'affect', 'morrison', 'koala']\n",
      "REJECTED: \n",
      " ['oppose', 'backing', 'suport', 'supports', 'supprt', 'help', 'government', 'chief', 'rebuild', 'business', 'community', 'announce', 'today', 'need', 'auspol', 'people']\n",
      "------------------------------------------------------------------\n",
      "STARTING ITERATION: 1!\n",
      "------------------------------------------------------------------\n",
      "Loaded reference set of size 1682 in 1.89 seconds.\n",
      "Loaded search set of size 18713 in 4.11 seconds.\n",
      "Time to process corpus: 2.94 seconds\n",
      "\n",
      "4159 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 20395 by 4988 with 295035 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.33 seconds\n",
      "\n",
      "Ref training size: 1682; Search training size: 6175; Training size: 7857; Test size: 18713\n",
      "\n",
      "Time for Naive Bayes: 0.01 seconds\n",
      "Time for Logit: 0.18 seconds\n",
      "1666 documents in target set\n",
      "17047 documents in non-target set\n",
      "1195 target set keywords found\n",
      "1838 non-target set keywords found\n",
      "Keep FIRE yes or no? yes\n",
      "Look at FIRE's most similar word embeddings, yes or no? yes\n",
      "Keep embedding FIRES yes or no? yes\n",
      "Keep embedding FLAMES yes or no? yes\n",
      "Keep embedding THREE-ALARM yes or no? yes\n",
      "Keep embedding TWO-ALARM yes or no? no\n",
      "Keep embedding FOUR-ALARM yes or no? no\n",
      "Keep embedding BLAZE yes or no? yes\n",
      "Keep embedding FIVE-ALARM yes or no? \n",
      "Keep embedding FIRE. yes or no? \n",
      "Keep embedding CONFLAGRATION yes or no? \n",
      "Keep TYFYS yes or no? \n",
      "Keep DISASTER yes or no? \n",
      "Keep AREA yes or no? \n",
      "Keep YOURADF yes or no? \n",
      "Keep LOVEGIPPSLAND yes or no? \n",
      "Keep FLOOD yes or no? \n",
      "Do you wish to add any further keywords? If yes, Type keyword:  \n",
      "------------------------------------------------------------------\n",
      "                     CURRENT KEYWORDS AFTER ITTERATION 1\n",
      "------------------------------------------------------------------\n",
      "ACCEPTED: \n",
      " ['support', 'supporting', 'suppport', 'supported', 'recovery', 'affect', 'morrison', 'koala', 'fire', 'fires', 'flames', 'three-alarm', 'blaze']\n",
      "REJECTED: \n",
      " ['oppose', 'backing', 'suport', 'supports', 'supprt', 'help', 'government', 'chief', 'rebuild', 'business', 'community', 'announce', 'today', 'need', 'auspol', 'people', 'two-alarm', 'four-alarm']\n"
     ]
    }
   ],
   "source": [
    "keywords = query.get_keywords(2, top_n=10, refkeys=[\"#bushfire\", \"#bushfires\", \"bushfire\", \"bushfires\"], \n",
    "                                 tarkeys = [\"fire\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mlogit Model - Average Marginal Effect Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the key-words from Kings model to subset tweets. Next we use this subset to predict the party of the tweet and analyze the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlogitMargins:\n",
    "    \"\"\"\n",
    "    Calculates marginal effect of multinomial logit coefficients with bootstraped confidence interval.\n",
    "    See https://github.com/alicehwu/gendered_language/blob/master/gendered_language_2018.pdf for a \n",
    "    reference on a similar approach but with binary classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        #Define data\n",
    "        self.vect = CountVectorizer(ngram_range=(1,2), min_df = 5)\n",
    "        self.X = self.vect.fit_transform(X)\n",
    "        self.y = y\n",
    "        print(\"Fitting model and calculating margins...\")\n",
    "        #Fit model and calculate average marginal effects\n",
    "        self.margins, self.fitted_model = self.avg_margins(self.X, self.y)\n",
    "    \n",
    "    def avg_margins(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates average marginal effect of coefficients in multinomial logit model, where each coefficient is \n",
    "        a word token and is calculated as:\n",
    "        avg_margin_jk =  1/N * sum_i{sum{P(y_i = j) * [beta_jk - sum_m(P(y_i = m) * beta_jm)]} for each token k and class j.\n",
    "        -----------\n",
    "        Returns:\n",
    "            - DataFrame with average margins of shape classes_j * tokens_k\n",
    "            - The fitted mlogit model \n",
    "        \"\"\"\n",
    "        #Fit the model\n",
    "        model = self.fit_model(X, y)\n",
    "        #Get probabilities for each obs i belonging to class j. shape = j * N\n",
    "        probas = model.predict_proba(X)\n",
    "        #Get coefficients. Shape j_classes * k_coefficients\n",
    "        betas = model.coef_\n",
    "        \n",
    "        avg_margins = {}\n",
    "        for j in tqdm(range(len(model.classes_))):\n",
    "            for k in range(betas.shape[1]):\n",
    "                dydw_jk = 0 \n",
    "                for i in range(probas.shape[0]):\n",
    "                    dydw_jk +=  probas[i,j] * (betas[j,k] - sum([probas[i,m] * betas[m,k] for m in range(len(model.classes_))]))\n",
    "\n",
    "                avg_margin = 1 / probas.shape[0] * dydw_jk\n",
    "                if model.classes_[j] not in avg_margins.keys():\n",
    "                            avg_margins[model.classes_[j]] = [avg_margin]\n",
    "                else:\n",
    "                    avg_margins[model.classes_[j]].append(avg_margin)\n",
    "        \n",
    "        #Extract the token name corresponding to the avg margins   \n",
    "        avg_margins[\"token\"] = self.vect.get_feature_names()\n",
    "        margins_df = pd.DataFrame(avg_margins)\n",
    "        \n",
    "        return margins_df, model\n",
    "\n",
    "    def avg_margins2(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates average marginal effect of coefficients in multinomial logit model, where each coefficient is \n",
    "        a word token and is calculated as:\n",
    "        avg_margin_jk = beta_jk * 1/N * sum{P(y_i = J) * 1-P(y_i = J)} for each token k and class j.\n",
    "        See: https://math.stackexchange.com/questions/863258/deriving-marginal-effects-in-multinomial-logit-model\n",
    "        -----------\n",
    "        Returns:\n",
    "            - DataFrame with average margins of shape classes_j * tokens_k\n",
    "            - The fitted mlogit model \n",
    "        \"\"\"\n",
    "        #Fit the model\n",
    "        model = self.fit_model(X, y)\n",
    "        #Get probabilities for each obs i belonging to class j. shape = j * N\n",
    "        probas = model.predict_proba(X)\n",
    "        #Get coefficients. Shape j_classes * k_coefficients\n",
    "        betas = model.coef_\n",
    "\n",
    "        margins = {}\n",
    "        #Loop over each class \n",
    "        for class_j in range(len(model.classes_)):\n",
    "            #Extract corresponging betas. shape 1*k\n",
    "            for beta_jk in betas[class_j]:\n",
    "                #Calculate avg margins for the jth class and kth beta (token)\n",
    "                margins_jk = beta_jk * 1/probas.shape[0] * (probas[:,class_j] * (1 - probas[:,class_j])).sum()\n",
    "                #Append to dictionary\n",
    "                if model.classes_[class_j] not in margins.keys():\n",
    "                    margins[model.classes_[class_j]] = [margins_jk]\n",
    "                else:\n",
    "                    margins[model.classes_[class_j]].append(margins_jk)\n",
    "        \n",
    "        #Extract the token name corresponding to the avg margins   \n",
    "        margins[\"token\"] = self.vect.get_feature_names()\n",
    "        margins_df = pd.DataFrame(margins)\n",
    "        \n",
    "        return margins_df, model\n",
    "    \n",
    "    def fit_model(self, X, y, max_iter = 10000, penalty = \"l1\", class_weight = \"balanced\",\n",
    "                            verbose = False, fit_intercept = True, multi_class = \"multinomial\",\n",
    "                            C = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fits sklearn multinomial logistic model on data.\n",
    "        \"\"\"\n",
    "\n",
    "        mlogit = LogisticRegression(random_state=42, penalty=penalty, solver=\"saga\", \n",
    "                                                           max_iter = max_iter, class_weight = class_weight,\n",
    "                                                           fit_intercept=fit_intercept, multi_class=multi_class,\n",
    "                                                           verbose=verbose, C = C).fit(X, y)  \n",
    "\n",
    "        return mlogit\n",
    "    \n",
    "    def bootstrap_ci(self, alpha = 0.05, n_samples = 500, sample_prop = 0.4):\n",
    "        \"\"\"\n",
    "        Uses bootstraping to calculate confidence interval around average marginal effects estimate.\n",
    "        -------------\n",
    "        Arguments:\n",
    "            - alpha: deterimines confidence level of the interval\n",
    "            - n_samples: amount of bootstrap samples\n",
    "            - sample_prop: bootstramp sample size as proportion of original sample\n",
    "        -------------\n",
    "        Return:\n",
    "            - Pandas dataframe with confidence interval for average marginal effect\n",
    "              of each coefficient.\n",
    "        \"\"\"\n",
    "        statistics = [] #List for bootstrap results\n",
    "        \n",
    "        #Define the bootstrap sample size based on proportion of total\n",
    "        n_size = int(self.X.shape[0] * sample_prop)\n",
    "        print(f\"Number of obs in bootstrap samples {n_size}...\")\n",
    "        start = time.time()\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            #Draw random sample from X and y with replacement\n",
    "            idx = np.random.choice(np.arange(self.X.shape[0]), n_size, replace=True)\n",
    "            X_sample = self.X[idx]\n",
    "            y_sample = self.y[idx]\n",
    "            #Calculate marginal effect for sample\n",
    "            margins = self.avg_margins(X_sample, y_sample)[0]\n",
    "            statistics.append(margins)\n",
    "        \n",
    "        #Join the resulting dataframes of margins\n",
    "        statistics = pd.concat(statistics)\n",
    "        print(f\"Boostraping Done. Calculating {1-alpha}% confidence interval...\")\n",
    "        #From bootstrap results get lower and upper CI limits based on alpha\n",
    "        statistics = statistics.groupby(\"token\").quantile([alpha, 1-alpha]).reset_index()        \n",
    "        print(\"Time to complete: \", time.time() - start)\n",
    "\n",
    "        return statistics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This data is just for testing - not final subset.\n",
    "mlogit_data = data.loc[(data[\"lemmas\"].str.contains(\"bushfire|bushfires|#bushfire|#bushfires|fire|flames|firemen\") == True) & \n",
    "                                         (data[\"lemmas\"].str.contains(\"corona|covid\") == False) & \n",
    "                                         (data[\"created_at\"] >= \"2019-06-01\") & \n",
    "                                         (data[\"created_at\"] <= \"2020-06-01\")]\n",
    "\n",
    "#Reduce classes to predict. Otherwise bad model performance.\n",
    "mlogit_data = mlogit_data.loc[mlogit_data[\"party\"].isin([\"Australian Greens\",\n",
    "                                                                                                   \"Australian Labor Party\", \n",
    "                                                                                                   \"Liberal Party of Australia\",\n",
    "                                                                                                    ])].dropna(subset = [\"lemmas\"]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model and calculating margins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:33<00:00, 11.16s/it]\n"
     ]
    }
   ],
   "source": [
    "X, y = mlogit_data[\"lemmas\"], mlogit_data[\"party\"]\n",
    "mlogit = MlogitMargins(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>-0.038275</td>\n",
       "      <td>-0.211655</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>courage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>-0.037718</td>\n",
       "      <td>-0.208577</td>\n",
       "      <td>0.246294</td>\n",
       "      <td>initial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>-0.036854</td>\n",
       "      <td>-0.203797</td>\n",
       "      <td>0.240650</td>\n",
       "      <td>network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>-0.084561</td>\n",
       "      <td>-0.136502</td>\n",
       "      <td>0.221064</td>\n",
       "      <td>need take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.032217</td>\n",
       "      <td>-0.178154</td>\n",
       "      <td>0.210371</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>-0.031822</td>\n",
       "      <td>-0.175974</td>\n",
       "      <td>0.207796</td>\n",
       "      <td>telecommunication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>-0.030986</td>\n",
       "      <td>-0.171349</td>\n",
       "      <td>0.202335</td>\n",
       "      <td>ensure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>-0.030561</td>\n",
       "      <td>-0.168997</td>\n",
       "      <td>0.199558</td>\n",
       "      <td>join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>-0.029335</td>\n",
       "      <td>-0.162218</td>\n",
       "      <td>0.191552</td>\n",
       "      <td>defence force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.028565</td>\n",
       "      <td>-0.157960</td>\n",
       "      <td>0.186525</td>\n",
       "      <td>attend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>-0.028313</td>\n",
       "      <td>-0.156567</td>\n",
       "      <td>0.184880</td>\n",
       "      <td>tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>-0.028181</td>\n",
       "      <td>-0.155840</td>\n",
       "      <td>0.184021</td>\n",
       "      <td>onto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-0.027150</td>\n",
       "      <td>-0.150137</td>\n",
       "      <td>0.177287</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>0.118437</td>\n",
       "      <td>-0.294950</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>volunteer fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>-0.026975</td>\n",
       "      <td>-0.149171</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.023827</td>\n",
       "      <td>-0.131761</td>\n",
       "      <td>0.155588</td>\n",
       "      <td>centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>-0.023801</td>\n",
       "      <td>-0.131616</td>\n",
       "      <td>0.155417</td>\n",
       "      <td>mental health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.033766</td>\n",
       "      <td>-0.118330</td>\n",
       "      <td>0.152097</td>\n",
       "      <td>thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.018526</td>\n",
       "      <td>-0.162781</td>\n",
       "      <td>0.144254</td>\n",
       "      <td>back foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>-0.021964</td>\n",
       "      <td>-0.121457</td>\n",
       "      <td>0.143420</td>\n",
       "      <td>currently</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Australian Greens  Australian Labor Party  Liberal Party of Australia  \\\n",
       "385           -0.038275               -0.211655                    0.249929   \n",
       "836           -0.037718               -0.208577                    0.246294   \n",
       "1054          -0.036854               -0.203797                    0.240650   \n",
       "1053          -0.084561               -0.136502                    0.221064   \n",
       "17            -0.032217               -0.178154                    0.210371   \n",
       "1466          -0.031822               -0.175974                    0.207796   \n",
       "528           -0.030986               -0.171349                    0.202335   \n",
       "859           -0.030561               -0.168997                    0.199558   \n",
       "431           -0.029335               -0.162218                    0.191552   \n",
       "109           -0.028565               -0.157960                    0.186525   \n",
       "1464          -0.028313               -0.156567                    0.184880   \n",
       "1093          -0.028181               -0.155840                    0.184021   \n",
       "281           -0.027150               -0.150137                    0.177287   \n",
       "1586           0.118437               -0.294950                    0.176514   \n",
       "1429          -0.026975               -0.149171                    0.176147   \n",
       "298           -0.023827               -0.131761                    0.155588   \n",
       "988           -0.023801               -0.131616                    0.155417   \n",
       "1495          -0.033766               -0.118330                    0.152097   \n",
       "139            0.018526               -0.162781                    0.144254   \n",
       "404           -0.021964               -0.121457                    0.143420   \n",
       "\n",
       "                  token  \n",
       "385             courage  \n",
       "836             initial  \n",
       "1054            network  \n",
       "1053          need take  \n",
       "17           additional  \n",
       "1466  telecommunication  \n",
       "528              ensure  \n",
       "859                join  \n",
       "431       defence force  \n",
       "109              attend  \n",
       "1464                tax  \n",
       "1093               onto  \n",
       "281              canada  \n",
       "1586     volunteer fire  \n",
       "1429              story  \n",
       "298              centre  \n",
       "988       mental health  \n",
       "1495            thought  \n",
       "139           back foot  \n",
       "404           currently  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlogit.margins.sort_values(\"Liberal Party of Australia\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.346922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>climateemergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.178290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.178607</td>\n",
       "      <td>coal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.167382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045300</td>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.158005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>0.155544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0.155027</td>\n",
       "      <td>-0.053213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>port</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.154716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.136338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.133839</td>\n",
       "      <td>-0.037640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fight bushfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.121663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>literally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.119233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.119554</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.118395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.116694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>australia bushfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>0.114411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>survivor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.111020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>arctic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0.108200</td>\n",
       "      <td>-0.244920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.106455</td>\n",
       "      <td>-0.144245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0.104464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>0.103161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.098024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>climate emergency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Australian Greens  Australian Labor Party  Liberal Party of Australia  \\\n",
       "332            0.346922                0.000000                    0.000000   \n",
       "336            0.178290                0.000000                   -0.178607   \n",
       "325            0.167382                0.000000                   -0.045300   \n",
       "203            0.158005                0.000000                    0.000000   \n",
       "1533           0.155544                0.000000                    0.000000   \n",
       "1161           0.155027               -0.053213                    0.000000   \n",
       "150            0.154716                0.000000                    0.000000   \n",
       "405            0.136338                0.000000                    0.000000   \n",
       "594            0.133839               -0.037640                    0.000000   \n",
       "912            0.121663                0.000000                    0.000000   \n",
       "141            0.119233                0.000000                   -0.119554   \n",
       "292            0.118395                0.000000                    0.000000   \n",
       "117            0.116694                0.000000                    0.000000   \n",
       "1452           0.114411                0.000000                    0.000000   \n",
       "91             0.111020                0.000000                    0.000000   \n",
       "1307           0.108200               -0.244920                    0.000000   \n",
       "561            0.106455               -0.144245                    0.000000   \n",
       "869            0.104464                0.000000                    0.000000   \n",
       "1392           0.103161                0.000000                    0.000000   \n",
       "330            0.098024                0.000000                    0.000000   \n",
       "\n",
       "                    token  \n",
       "332      climateemergency  \n",
       "336                  coal  \n",
       "325               climate  \n",
       "203              building  \n",
       "1533                 town  \n",
       "1161                 port  \n",
       "150                 beach  \n",
       "405                   cut  \n",
       "594       fight bushfires  \n",
       "912             literally  \n",
       "141                   bad  \n",
       "292                  case  \n",
       "117   australia bushfires  \n",
       "1452             survivor  \n",
       "91                 arctic  \n",
       "1307            satellite  \n",
       "561            experience  \n",
       "869                  kill  \n",
       "1392            something  \n",
       "330     climate emergency  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlogit.margins.sort_values(\"Australian Greens\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df = 5)\n",
    "X = vect.fit_transform(X)\n",
    "feature_names = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X, y, max_iter = 10000, penalty = \"l1\", class_weight = \"balanced\",\n",
    "                            verbose = False, fit_intercept = True, multi_class = \"multinomial\",\n",
    "                            C = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fits sklearn multinomial logistic model on data.\n",
    "        \"\"\"\n",
    "    \n",
    "        \n",
    "        mlogit = LogisticRegression(random_state=42, penalty=penalty, solver=\"saga\", \n",
    "                                                           max_iter = max_iter, class_weight = class_weight,\n",
    "                                                           fit_intercept=fit_intercept, multi_class=multi_class,\n",
    "                                                           verbose=verbose, C = C).fit(X, y)  \n",
    "        return mlogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogit  = fit_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = mlogit.predict_proba(X)\n",
    "betas = mlogit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2044, 3) (3, 1658)\n"
     ]
    }
   ],
   "source": [
    "print(probas.shape, betas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2044, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:31<00:00, 10.56s/it]\n"
     ]
    }
   ],
   "source": [
    "avg_margins = {}\n",
    "for j in tqdm(range(len(mlogit.classes_))):\n",
    "    for k in range(betas.shape[1]):\n",
    "        dydx = 0 \n",
    "        for i in range(probas.shape[0]):\n",
    "            dydx +=  probas[i,j] * (betas[j,k] - sum([probas[i,m] * betas[m,k] \n",
    "                                                      for m in range(len(mlogit.classes_))]))\n",
    "        \n",
    "        avg_margin = 1 / probas.shape[0] * dydx\n",
    "        if mlogit.classes_[j] not in avg_margins.keys():\n",
    "                    avg_margins[mlogit.classes_[j]] = [avg_margin]\n",
    "        else:\n",
    "            avg_margins[mlogit.classes_[j]].append(avg_margin)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(avg_margins)\n",
    "test_df[\"term\"] = feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.346922</td>\n",
       "      <td>-0.273272</td>\n",
       "      <td>-0.073650</td>\n",
       "      <td>climateemergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0.206547</td>\n",
       "      <td>-0.330150</td>\n",
       "      <td>0.123603</td>\n",
       "      <td>satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.205642</td>\n",
       "      <td>0.010815</td>\n",
       "      <td>-0.216457</td>\n",
       "      <td>coal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0.176395</td>\n",
       "      <td>-0.175329</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>port</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.174319</td>\n",
       "      <td>-0.093485</td>\n",
       "      <td>-0.080834</td>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.164376</td>\n",
       "      <td>-0.228100</td>\n",
       "      <td>0.063724</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.158005</td>\n",
       "      <td>-0.124461</td>\n",
       "      <td>-0.033544</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0.155771</td>\n",
       "      <td>-0.270009</td>\n",
       "      <td>0.114238</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>0.155544</td>\n",
       "      <td>-0.122523</td>\n",
       "      <td>-0.033021</td>\n",
       "      <td>town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.154716</td>\n",
       "      <td>-0.121871</td>\n",
       "      <td>-0.032845</td>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.148953</td>\n",
       "      <td>-0.143066</td>\n",
       "      <td>-0.005887</td>\n",
       "      <td>fight bushfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.137542</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>-0.144867</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.136338</td>\n",
       "      <td>-0.107394</td>\n",
       "      <td>-0.028944</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.123011</td>\n",
       "      <td>-0.211021</td>\n",
       "      <td>0.088010</td>\n",
       "      <td>discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.121663</td>\n",
       "      <td>-0.095834</td>\n",
       "      <td>-0.025828</td>\n",
       "      <td>literally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Australian Greens  Australian Labor Party  Liberal Party of Australia  \\\n",
       "332            0.346922               -0.273272                   -0.073650   \n",
       "1307           0.206547               -0.330150                    0.123603   \n",
       "336            0.205642                0.010815                   -0.216457   \n",
       "1161           0.176395               -0.175329                   -0.001066   \n",
       "325            0.174319               -0.093485                   -0.080834   \n",
       "561            0.164376               -0.228100                    0.063724   \n",
       "203            0.158005               -0.124461                   -0.033544   \n",
       "738            0.155771               -0.270009                    0.114238   \n",
       "1533           0.155544               -0.122523                   -0.033021   \n",
       "150            0.154716               -0.121871                   -0.032845   \n",
       "594            0.148953               -0.143066                   -0.005887   \n",
       "141            0.137542                0.007325                   -0.144867   \n",
       "405            0.136338               -0.107394                   -0.028944   \n",
       "469            0.123011               -0.211021                    0.088010   \n",
       "912            0.121663               -0.095834                   -0.025828   \n",
       "\n",
       "                  term  \n",
       "332   climateemergency  \n",
       "1307         satellite  \n",
       "336               coal  \n",
       "1161              port  \n",
       "325            climate  \n",
       "561         experience  \n",
       "203           building  \n",
       "738              green  \n",
       "1533              town  \n",
       "150              beach  \n",
       "594    fight bushfires  \n",
       "141                bad  \n",
       "405                cut  \n",
       "469         discussion  \n",
       "912          literally  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sort_values(\"Australian Greens\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1658"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in betas[2]:\n",
    "    count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = {}\n",
    "        #Loop over each class \n",
    "        for class_j in range(len(model.classes_)):\n",
    "            #Extract corresponging betas. shape 1*k\n",
    "            for beta_jk in betas[class_j]:\n",
    "                #Calculate avg margins for the jth class and kth beta (token)\n",
    "                margins_jk = beta_jk * 1/probas.shape[0] * (probas[:,class_j] * (1 - probas[:,class_j])).sum()\n",
    "                #Append to dictionary\n",
    "                if model.classes_[class_j] not in margins.keys():\n",
    "                    margins[model.classes_[class_j]] = [margins_jk]\n",
    "                else:\n",
    "                    margins[model.classes_[class_j]].append(margins_jk)\n",
    "        \n",
    "        #Extract the token name corresponding to the avg margins   \n",
    "        margins[\"token\"] = self.vect.get_feature_names()\n",
    "        margins_df = pd.DataFrame(margins)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd379f7bb8e06f7ec29e5b7f14bfd985640a300b5a41de1996cc2754af32f7b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
