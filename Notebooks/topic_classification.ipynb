{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the output from the topic model and our immersion journal/manual to subset a training set of tweets about the Australian Bushfires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm #to create a progress bar\n",
    "#Machine learning packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Word embeddings\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "#Gary King et. al key-words\n",
    "from keyword_algorithm import *\n",
    "#Remove unwarranted warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gary Kings semi-automated keyword retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and prepare data\n",
    "data = pd.read_csv(\"data/final_df.csv\", index_col=0)\n",
    "data = data.dropna(subset = [\"lemmas\"]).reset_index(drop = True)\n",
    "data[\"index_col\"] = data.index\n",
    "#data.to_csv(\"data/query_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download model from https://fasttext.cc/docs/en/english-vectors.html\n",
    "model_dir = \"data/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryBuilder:\n",
    "    \n",
    "    def __init__(self, emb_model):\n",
    "        \n",
    "        self.query = Keywords()\n",
    "        #Load the data. Change path if necessary\n",
    "        path = 'data/query_df.csv'\n",
    "        self.query.LoadDataset(path, text_colname='lemmas', \n",
    "                    date_colname=\"created_at\", id_colname=\"index_col\")\n",
    "        #Load Gensim word embeddings model \n",
    "        self.we = emb_model\n",
    "            \n",
    "    def get_keywords(self, its = 2, top_n = 10, refkeys = [], tarkeys = [], algorithms = ['nbayes', 'logit'], \n",
    "                     date_start = \"2019-06-01\", date_end = \"2020-05-30\"):\n",
    "        \"\"\"\n",
    "        Loops over King. et. al algorithm to extract relevant keywords used for building a \n",
    "        boolean query to subset relevant Tweets in the dataset.\n",
    "        ---------\n",
    "        arguments:\n",
    "            - its: Iterations to run the algorithm\n",
    "            - top_n: integer of how many of the most predictive keywords to extract in each iteration\n",
    "            - refkeys: list of initial keywords used to create reference set of tweets\n",
    "            - tarkeys: list of initial keywords used to limit the search set\n",
    "            - algorithms: list of classifiers to run for extracting keywords \n",
    "            - date_start: y/m/d of start date for relevant tweets\n",
    "            - date_end: y/m/d of end date for relevant tweets\n",
    "        -----------    \n",
    "        returns:\n",
    "            - dictionary of accepted, rejected and nontarget keywords\n",
    "        \"\"\"\n",
    "        \n",
    "        accepted_keywords = []\n",
    "        rejected_keywords = []\n",
    "        nontarget_keywords = []\n",
    "        \n",
    "        #Begin loop for mining search set\n",
    "        for it in range(its):\n",
    "            print(\"-\"*66)\n",
    "            print(f\"STARTING ITERATION: {it}!\")\n",
    "            if it == 0:\n",
    "                print(\"INITIAL REFERENCE KEYS: {refkeys} \\n INITIAL TARGET KEYS: {tarkeys}\")\n",
    "            print(\"-\"*66)\n",
    "            \n",
    "            #Build reference set of tweets\n",
    "            self.query.ReferenceSet(any_words=refkeys, date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            #Use accepted keys as search keys if not the first iteration\n",
    "            if it > 0:\n",
    "                self.query.SearchSet(any_words = accepted_keywords, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            else:\n",
    "                self.query.SearchSet(any_words = tarkeys, \n",
    "                                     date_start=date_start, date_end=date_end)\n",
    "            \n",
    "            \n",
    "            #Run King algorithm to find keywords.\n",
    "            self.query.ProcessData(stem = False, keep_twitter_symbols=False,\n",
    "                                   remove_wordlist=refkeys)\n",
    "            self.query.ReferenceKeywords()\n",
    "            self.query.ClassifyDocs(min_df=5, ref_trainprop=1, algorithms=algorithms)\n",
    "            self.query.FindTargetSet()\n",
    "            self.query.FindKeywords()\n",
    "            \n",
    "            #Extract target keywords from algorithm results\n",
    "            target_keywords = self.query.target_keywords[:top_n]\n",
    "            #Also get the reference set keywords to loop over\n",
    "            target_keywords += self.query.reference_keywords[:top_n]\n",
    "            #Append unique nontarget keywords to list of nontarget keys\n",
    "            for nonkey in self.query.nontarget_keywords[:100]:\n",
    "                if nonkey not in nontarget_keywords:\n",
    "                    nontarget_keywords.append(nonkey)\n",
    "            \n",
    "            #Loop over each relevant keyword from reference and found target keywords\n",
    "            for keyword in target_keywords:\n",
    "                #Check if keyword has already been rejected or accepted\n",
    "                if keyword in accepted_keywords or keyword in rejected_keywords:\n",
    "                    continue\n",
    "                else:\n",
    "                    inp = input(f\"Keep {keyword.upper()} yes or no?\")\n",
    "                    if inp == \"yes\":\n",
    "                        accepted_keywords.append(keyword)\n",
    "                        #get similar keywords through most similar pretrained embeddings\n",
    "                        inp2 = input(f\"Look at {keyword.upper()}'s most similar word embeddings, yes or no?\")\n",
    "                        if inp2 == \"yes\":\n",
    "                            #Look if keyword exist in embedding model dictionary\n",
    "                            try:\n",
    "                                embeddings = [emb[0] for emb in self.we.most_similar(keyword)]\n",
    "                                for emb in embeddings:\n",
    "                                    if emb.lower() in accepted_keywords or emb.lower() in rejected_keywords:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        inp3 = input(f\"Keep embedding {emb.upper()} yes or no?\")\n",
    "                                        if inp3 == \"yes\":\n",
    "                                            accepted_keywords.append(emb)\n",
    "                                        elif inp3 ==\"no\":\n",
    "                                            rejected_keywords.append(emb)\n",
    "                            except:\n",
    "                                print(f\"{keyword.upper()} embedding not present in Model!\")\n",
    "                                pass\n",
    "                        elif inp2 == \"no\":\n",
    "                            pass\n",
    "                    elif inp == \"no\":\n",
    "                        rejected_keywords.append(keyword)\n",
    "                        \n",
    "            #Add custom keyword(s) in the end of the loop. Either as list or single keyword\n",
    "            inp4 = input(f\"Do you wish to add any further keywords? If yes, Type keyword: \")\n",
    "            if inp4:\n",
    "                if isinstance(inp4, list):\n",
    "                    [accepted_keywords.append(key) for key in inp4]\n",
    "                else:\n",
    "                     accepted_keywords.append(inp4)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            print(\"-\"*66)\n",
    "            print(\" \"*20, f\"CURRENT KEYWORDS AFTER ITTERATION {it}\")\n",
    "            print(\"-\"*66)\n",
    "            print(f\"ACCEPTED: \\n {accepted_keywords}\")\n",
    "            print(f\"REJECTED: \\n {rejected_keywords}\")\n",
    "            \n",
    "        \n",
    "        keywords = {\"accepted_keys\":accepted_keywords, \"rejected_keys\":rejected_keywords,\"nontarget_keys\":nontarget_keywords}\n",
    "        \n",
    "        return keywords\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword object initialized.\n",
      "Loaded corpus of size 148540 in 2.21 seconds.\n"
     ]
    }
   ],
   "source": [
    "query = QueryBuilder(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accepted_keys': ['support',\n",
       "  'supporting',\n",
       "  'suppport',\n",
       "  'supported',\n",
       "  'recovery',\n",
       "  'affect',\n",
       "  'morrison',\n",
       "  'koala',\n",
       "  'fire',\n",
       "  'fires',\n",
       "  'flames',\n",
       "  'three-alarm',\n",
       "  'blaze'],\n",
       " 'rejected_keys': ['oppose',\n",
       "  'backing',\n",
       "  'suport',\n",
       "  'supports',\n",
       "  'supprt',\n",
       "  'help',\n",
       "  'government',\n",
       "  'chief',\n",
       "  'rebuild',\n",
       "  'business',\n",
       "  'community',\n",
       "  'announce',\n",
       "  'today',\n",
       "  'need',\n",
       "  'auspol',\n",
       "  'people',\n",
       "  'two-alarm',\n",
       "  'four-alarm'],\n",
       " 'nontarget_keys': ['canberratimes',\n",
       "  'coal',\n",
       "  'via',\n",
       "  'station',\n",
       "  'north',\n",
       "  'ban',\n",
       "  'contain',\n",
       "  'situation',\n",
       "  'front',\n",
       "  'volunteer',\n",
       "  'activity',\n",
       "  'qld',\n",
       "  'power',\n",
       "  'near',\n",
       "  'total',\n",
       "  'train',\n",
       "  'tomorrow',\n",
       "  'woman',\n",
       "  'hour',\n",
       "  'fight',\n",
       "  'morning',\n",
       "  'gas',\n",
       "  'right',\n",
       "  'burn',\n",
       "  'declare',\n",
       "  'energy',\n",
       "  'unprecedented',\n",
       "  'think',\n",
       "  'yesterday',\n",
       "  'hot',\n",
       "  'hill',\n",
       "  'friend',\n",
       "  'close',\n",
       "  'place',\n",
       "  'time',\n",
       "  'news',\n",
       "  'firefighter',\n",
       "  'green',\n",
       "  'rain',\n",
       "  'congratulation',\n",
       "  'gov',\n",
       "  'early',\n",
       "  'line',\n",
       "  'dangerous',\n",
       "  'wind',\n",
       "  'fighter',\n",
       "  'everyone',\n",
       "  'extraordinary',\n",
       "  'ahead',\n",
       "  'control',\n",
       "  'west',\n",
       "  'evacuation',\n",
       "  'fly',\n",
       "  'men',\n",
       "  'direction',\n",
       "  'club',\n",
       "  'story',\n",
       "  'attack',\n",
       "  'blaze',\n",
       "  'like',\n",
       "  'tonight',\n",
       "  'act',\n",
       "  'loss',\n",
       "  'aircraft',\n",
       "  'night',\n",
       "  'range',\n",
       "  'water',\n",
       "  'firefighting',\n",
       "  'anyone',\n",
       "  'remember',\n",
       "  'nsw',\n",
       "  'large',\n",
       "  'bit',\n",
       "  'stop',\n",
       "  'threaten',\n",
       "  'weather',\n",
       "  'last',\n",
       "  'rescue',\n",
       "  'australias',\n",
       "  'saturday',\n",
       "  'creek',\n",
       "  'something',\n",
       "  'briefing',\n",
       "  'watch',\n",
       "  'northern',\n",
       "  'leadership',\n",
       "  'low',\n",
       "  'honour',\n",
       "  'several',\n",
       "  'kangaroo',\n",
       "  'hero',\n",
       "  'rfs',\n",
       "  'nswrfs',\n",
       "  'check',\n",
       "  'aerial',\n",
       "  'difficult',\n",
       "  'hard',\n",
       "  'danger',\n",
       "  'tough',\n",
       "  'good',\n",
       "  'morrison',\n",
       "  'scott',\n",
       "  'government',\n",
       "  'auspol',\n",
       "  'job',\n",
       "  'labor',\n",
       "  'age',\n",
       "  'cut',\n",
       "  'jobkeeper',\n",
       "  'liberal',\n",
       "  'worker',\n",
       "  'plan',\n",
       "  'care',\n",
       "  'budget',\n",
       "  'policy',\n",
       "  'commission',\n",
       "  'parliament',\n",
       "  'australian',\n",
       "  'party',\n",
       "  'vote',\n",
       "  'wage',\n",
       "  'vaccine',\n",
       "  'bill',\n",
       "  'create',\n",
       "  'minister',\n",
       "  'want',\n",
       "  'law',\n",
       "  'system',\n",
       "  'must',\n",
       "  'student',\n",
       "  'economic',\n",
       "  'economy',\n",
       "  'end',\n",
       "  'tax',\n",
       "  'real',\n",
       "  'university',\n",
       "  'recession',\n",
       "  'would',\n",
       "  'million',\n",
       "  'public',\n",
       "  'doesnt',\n",
       "  'say',\n",
       "  'unemployment',\n",
       "  'could',\n",
       "  'instead',\n",
       "  'house',\n",
       "  'kelly',\n",
       "  'robodebt',\n",
       "  'program',\n",
       "  'nothing',\n",
       "  'prime',\n",
       "  'treasurer',\n",
       "  'education',\n",
       "  'mean',\n",
       "  'old',\n",
       "  'palmer',\n",
       "  'cancer',\n",
       "  'veteran',\n",
       "  'price',\n",
       "  'abbott',\n",
       "  'wont',\n",
       "  'taxpayer',\n",
       "  'royal',\n",
       "  'failure',\n",
       "  'growth',\n",
       "  'turnbull',\n",
       "  'motion',\n",
       "  'rate',\n",
       "  'young',\n",
       "  'election',\n",
       "  'clive',\n",
       "  'jobseeker',\n",
       "  'industry',\n",
       "  'enough',\n",
       "  'scheme',\n",
       "  'childcare',\n",
       "  'nation',\n",
       "  'announcement',\n",
       "  'secure',\n",
       "  'invest',\n",
       "  'domestic',\n",
       "  'super',\n",
       "  'emission',\n",
       "  'record',\n",
       "  'guarantee',\n",
       "  'pass',\n",
       "  'integrity',\n",
       "  'campaign',\n",
       "  'sector',\n",
       "  'fail',\n",
       "  'manufacturing',\n",
       "  'question',\n",
       "  'try',\n",
       "  'choose',\n",
       "  'grow']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "STARTING ITERATION: 0!\n",
      "INITIAL REFERENCE KEYS: {refkeys} \n",
      " INITIAL TARGET KEYS: {tarkeys}\n",
      "------------------------------------------------------------------\n",
      "Loaded reference set of size 1682 in 2.09 seconds.\n",
      "Loaded search set of size 1368 in 0.52 seconds.\n",
      "Time to process corpus: 0.49 seconds\n",
      "\n",
      "4159 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 3050 by 1495 with 37198 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.05 seconds\n",
      "\n",
      "Ref training size: 1682; Search training size: 451; Training size: 2133; Test size: 1368\n",
      "\n",
      "Time for Naive Bayes: 0.01 seconds\n",
      "Time for Logit: 0.04 seconds\n",
      "473 documents in target set\n",
      "895 documents in non-target set\n",
      "253 target set keywords found\n",
      "185 non-target set keywords found\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Keep SUPPORT yes or no? yes\n",
      "Look at SUPPORT's most similar word embeddings, yes or no? yes\n",
      "Keep embedding SUPPORTING yes or no? yes\n",
      "Keep embedding SUPPPORT yes or no? yes\n",
      "Keep embedding SUPPORTED yes or no? yes\n",
      "Keep embedding OPPOSE yes or no? no\n",
      "Keep embedding BACKING yes or no? no\n",
      "Keep embedding SUPORT yes or no? no\n",
      "Keep embedding SUPPORTS yes or no? no\n",
      "Keep embedding SUPPRT yes or no? no\n",
      "Keep embedding HELP yes or no? no\n",
      "Keep GOVERNMENT yes or no? no\n",
      "Keep RECOVERY yes or no? yes\n",
      "Look at RECOVERY's most similar word embeddings, yes or no? no\n",
      "Keep AFFECT yes or no? yes\n",
      "Look at AFFECT's most similar word embeddings, yes or no? no\n",
      "Keep MORRISON yes or no? yes\n",
      "Look at MORRISON's most similar word embeddings, yes or no? no\n",
      "Keep CHIEF yes or no? no\n",
      "Keep REBUILD yes or no? no\n",
      "Keep BUSINESS yes or no? no\n",
      "Keep COMMUNITY yes or no? no\n",
      "Keep ANNOUNCE yes or no? no\n",
      "Keep TODAY yes or no? no\n",
      "Keep NEED yes or no? no\n",
      "Keep AUSPOL yes or no? no\n",
      "Keep PEOPLE yes or no? no\n",
      "Do you wish to add any further keywords? If yes, Type keyword:  koala\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "                     CURRENT KEYWORDS AFTER ITTERATION 0\n",
      "------------------------------------------------------------------\n",
      "ACCEPTED: \n",
      " ['support', 'supporting', 'suppport', 'supported', 'recovery', 'affect', 'morrison', 'koala']\n",
      "REJECTED: \n",
      " ['oppose', 'backing', 'suport', 'supports', 'supprt', 'help', 'government', 'chief', 'rebuild', 'business', 'community', 'announce', 'today', 'need', 'auspol', 'people']\n",
      "------------------------------------------------------------------\n",
      "STARTING ITERATION: 1!\n",
      "------------------------------------------------------------------\n",
      "Loaded reference set of size 1682 in 1.89 seconds.\n",
      "Loaded search set of size 18713 in 4.11 seconds.\n",
      "Time to process corpus: 2.94 seconds\n",
      "\n",
      "4159 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 20395 by 4988 with 295035 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.33 seconds\n",
      "\n",
      "Ref training size: 1682; Search training size: 6175; Training size: 7857; Test size: 18713\n",
      "\n",
      "Time for Naive Bayes: 0.01 seconds\n",
      "Time for Logit: 0.18 seconds\n",
      "1666 documents in target set\n",
      "17047 documents in non-target set\n",
      "1195 target set keywords found\n",
      "1838 non-target set keywords found\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Keep FIRE yes or no? yes\n",
      "Look at FIRE's most similar word embeddings, yes or no? yes\n",
      "Keep embedding FIRES yes or no? yes\n",
      "Keep embedding FLAMES yes or no? yes\n",
      "Keep embedding THREE-ALARM yes or no? yes\n",
      "Keep embedding TWO-ALARM yes or no? no\n",
      "Keep embedding FOUR-ALARM yes or no? no\n",
      "Keep embedding BLAZE yes or no? yes\n",
      "Keep embedding FIVE-ALARM yes or no? \n",
      "Keep embedding FIRE. yes or no? \n",
      "Keep embedding CONFLAGRATION yes or no? \n",
      "Keep TYFYS yes or no? \n",
      "Keep DISASTER yes or no? \n",
      "Keep AREA yes or no? \n",
      "Keep YOURADF yes or no? \n",
      "Keep LOVEGIPPSLAND yes or no? \n",
      "Keep FLOOD yes or no? \n",
      "Do you wish to add any further keywords? If yes, Type keyword:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "                     CURRENT KEYWORDS AFTER ITTERATION 1\n",
      "------------------------------------------------------------------\n",
      "ACCEPTED: \n",
      " ['support', 'supporting', 'suppport', 'supported', 'recovery', 'affect', 'morrison', 'koala', 'fire', 'fires', 'flames', 'three-alarm', 'blaze']\n",
      "REJECTED: \n",
      " ['oppose', 'backing', 'suport', 'supports', 'supprt', 'help', 'government', 'chief', 'rebuild', 'business', 'community', 'announce', 'today', 'need', 'auspol', 'people', 'two-alarm', 'four-alarm']\n"
     ]
    }
   ],
   "source": [
    "keywords = query.get_keywords(2, top_n=10, refkeys=[\"#bushfire\", \"#bushfires\", \"bushfire\", \"bushfires\"], \n",
    "                                 tarkeys = [\"fire\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mlogit Model - Average Marginal Effect Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the key-words from Kings model to subset tweets. Next we use this subset to predict the party of the tweet and analyze the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlogitMargins:\n",
    "    \"\"\"\n",
    "    Calculates marginal effect of multinomial logit coefficients with bootstraped confidence interval.\n",
    "    See https://github.com/alicehwu/gendered_language/blob/master/gendered_language_2018.pdf for a \n",
    "    reference on a similar approach but with binary classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        #Define data\n",
    "        self.vect = CountVectorizer(ngram_range=(1,2), min_df = 5)\n",
    "        self.X = self.vect.fit_transform(X)\n",
    "        self.y = y\n",
    "        print(\"Fitting model and calculating margins...\")\n",
    "        #Fit model and calculate average marginal effects\n",
    "        self.margins, self.fitted_model = self.avg_margins(self.X, self.y)\n",
    "        \n",
    "\n",
    "    def avg_margins(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates average marginal effect of coefficients in multinomial logit model, where each coefficient is \n",
    "        a word token and is calculated as:\n",
    "        avg_margin_jk = beta_jk * 1/N * sum{P(y_i = J) * 1-P(y_i = J)} for each token k and class j.\n",
    "        See: https://math.stackexchange.com/questions/863258/deriving-marginal-effects-in-multinomial-logit-model\n",
    "        -----------\n",
    "        Returns:\n",
    "            - DataFrame with average margins of shape classes_j * tokens_k\n",
    "            - The fitted mlogit model \n",
    "        \"\"\"\n",
    "        #Fit the model\n",
    "        model = self.fit_model(X, y)\n",
    "        #Get probabilities for each obs i belonging to class j. shape = j * N\n",
    "        probas = model.predict_proba(X)\n",
    "        #Get coefficients. Shape j_classes * k_coefficients\n",
    "        betas = model.coef_\n",
    "\n",
    "        margins = {}\n",
    "        #Loop over each class \n",
    "        for class_j in range(len(model.classes_)):\n",
    "            #Extract corresponging betas. shape 1*k\n",
    "            for beta_jk in betas[class_j]:\n",
    "                \n",
    "                #Calculate avg margins for the jth class and kth beta (token)\n",
    "                margins_jk = beta_jk * 1/probas.shape[0] * (probas[:,class_j] * (1 - probas[:,class_j])).sum()\n",
    "                \n",
    "                #Append to dictionary\n",
    "                if model.classes_[class_j] not in margins.keys():\n",
    "                    margins[model.classes_[class_j]] = [margins_jk]\n",
    "                else:\n",
    "                    margins[model.classes_[class_j]].append(margins_jk)\n",
    "        \n",
    "        #Extract the token name corresponding to the avg margins   \n",
    "        margins[\"token\"] = self.vect.get_feature_names()\n",
    "        margins_df = pd.DataFrame(margins)\n",
    "        \n",
    "        return margins_df, model\n",
    "    \n",
    "    def fit_model(self, X, y, max_iter = 10000, penalty = \"l1\", class_weight = \"balanced\",\n",
    "                  verbose = False, fit_intercept = True, multi_class = \"multinomial\",\n",
    "                  C = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fits sklearn multinomial logistic model on data.\n",
    "        \"\"\"\n",
    "\n",
    "        mlogit = LogisticRegression(random_state=42, penalty=penalty, solver=\"saga\", \n",
    "                                    max_iter = max_iter, class_weight = class_weight,\n",
    "                                    fit_intercept=fit_intercept, multi_class=multi_class,\n",
    "                                    verbose=verbose, C = C).fit(X, y)  \n",
    "\n",
    "        return mlogit\n",
    "    \n",
    "    def bootstrap_ci(self, alpha = 0.05, n_samples = 500, sample_prop = 0.4):\n",
    "        \"\"\"\n",
    "        Uses bootstraping to calculate confidence interval around average marginal effects estimate.\n",
    "        -------------\n",
    "        Arguments:\n",
    "            - alpha: deterimines confidence level of the interval\n",
    "            - n_samples: amount of bootstrap samples\n",
    "            - sample_prop: bootstramp sample size as proportion of original sample\n",
    "        -------------\n",
    "        Return:\n",
    "            - Pandas dataframe with confidence interval for average marginal effect\n",
    "              of each coefficient.\n",
    "        \"\"\"\n",
    "        statistics = [] #List for bootstrap results\n",
    "        \n",
    "        #Define the bootstrap sample size based on proportion of total\n",
    "        n_size = int(self.X.shape[0] * sample_prop)\n",
    "        print(f\"Number of obs in bootstrap samples {n_size}...\")\n",
    "        start = time.time()\n",
    "        for i in tqdm(range(n_samples)):\n",
    "\n",
    "            #Draw random sample from X and y with replacement\n",
    "            idx = np.random.choice(np.arange(self.X.shape[0]), n_size, replace=True)\n",
    "            X_sample = self.X[idx]\n",
    "            y_sample = self.y[idx]\n",
    "            #Calculate marginal effect for sample\n",
    "            margins = self.avg_margins(X_sample, y_sample)[0]\n",
    "            statistics.append(margins)\n",
    "        \n",
    "        #Join the resulting dataframes of margins\n",
    "        statistics = pd.concat(statistics)\n",
    "        print(f\"Boostraping Done. Calculating {1-alpha}% confidence interval...\")\n",
    "        #From bootstrap results get lower and upper CI limits based on alpha\n",
    "        statistics = statistics.groupby(\"token\").quantile([alpha, 1-alpha]).reset_index()        \n",
    "        print(\"Time to complete: \", time.time() - start)\n",
    "\n",
    "        return statistics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This data is just for testing - not final subset.\n",
    "mlogit_data = data.loc[(data[\"lemmas\"].str.contains(\"bushfire|bushfires|#bushfire|#bushfires|disaster|kaoala|fire|flames|firemen\") == True) & \n",
    "                       (data[\"lemmas\"].str.contains(\"corona|covid\") == False) & \n",
    "                       (data[\"created_at\"] >= \"2019-06-01\") & \n",
    "                       (data[\"created_at\"] <= \"2020-06-01\")]\n",
    "\n",
    "#Reduce classes to predict. Otherwise bad model performance.\n",
    "mlogit_data = mlogit_data.loc[mlogit_data[\"party\"].isin([\"Australian Greens\",\n",
    "                                                         \"Australian Labor Party\", \n",
    "                                                         \"Liberal Party of Australia\",\n",
    "                                                         \"The Nationals\",\n",
    "                                                         \"Centre Alliance\"\n",
    "                                                          ])].dropna(subset = [\"lemmas\"]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model and calculating margins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matiasp/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "X, y = mlogit_data[\"lemmas\"], mlogit_data[\"party\"]\n",
    "mlogit = MlogitMargins(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Centre Alliance</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "      <th>The Nationals</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>-0.012168</td>\n",
       "      <td>-0.111829</td>\n",
       "      <td>-0.002200</td>\n",
       "      <td>-0.074854</td>\n",
       "      <td>0.120708</td>\n",
       "      <td>lovegippsland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>-0.013101</td>\n",
       "      <td>-0.084058</td>\n",
       "      <td>-0.001108</td>\n",
       "      <td>-0.072949</td>\n",
       "      <td>0.103549</td>\n",
       "      <td>tyfys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>-0.016448</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>-0.001566</td>\n",
       "      <td>-0.045663</td>\n",
       "      <td>0.058891</td>\n",
       "      <td>rf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>-0.015504</td>\n",
       "      <td>-0.014656</td>\n",
       "      <td>-0.001676</td>\n",
       "      <td>-0.030716</td>\n",
       "      <td>0.049876</td>\n",
       "      <td>gippsland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>-0.004280</td>\n",
       "      <td>-0.051423</td>\n",
       "      <td>-0.000895</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>0.049726</td>\n",
       "      <td>waggawagga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>-0.021562</td>\n",
       "      <td>-0.029050</td>\n",
       "      <td>-0.002463</td>\n",
       "      <td>-0.001055</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>-0.008681</td>\n",
       "      <td>-0.025396</td>\n",
       "      <td>-0.001055</td>\n",
       "      <td>-0.028007</td>\n",
       "      <td>0.043208</td>\n",
       "      <td>tumbarumba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-0.001146</td>\n",
       "      <td>-0.028951</td>\n",
       "      <td>-0.001928</td>\n",
       "      <td>-0.029873</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>anyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.059610</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>-0.017175</td>\n",
       "      <td>0.042380</td>\n",
       "      <td>near</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>-0.027180</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>-0.011814</td>\n",
       "      <td>0.041288</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-0.014152</td>\n",
       "      <td>-0.012463</td>\n",
       "      <td>-0.001291</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.009710</td>\n",
       "      <td>-0.050634</td>\n",
       "      <td>-0.001076</td>\n",
       "      <td>-0.007069</td>\n",
       "      <td>0.040397</td>\n",
       "      <td>affect fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>-0.009187</td>\n",
       "      <td>-0.040823</td>\n",
       "      <td>-0.003085</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.038058</td>\n",
       "      <td>opbushfireassist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>-0.010027</td>\n",
       "      <td>-0.046106</td>\n",
       "      <td>-0.000580</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>peter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>-0.008651</td>\n",
       "      <td>-0.079430</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>0.036207</td>\n",
       "      <td>telecommunication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>-0.003768</td>\n",
       "      <td>-0.047231</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.010833</td>\n",
       "      <td>0.035338</td>\n",
       "      <td>riverina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>-0.000704</td>\n",
       "      <td>-0.051690</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>-0.014767</td>\n",
       "      <td>0.034235</td>\n",
       "      <td>imagine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>-0.008359</td>\n",
       "      <td>-0.008282</td>\n",
       "      <td>-0.001552</td>\n",
       "      <td>-0.022005</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>deputy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>-0.005054</td>\n",
       "      <td>-0.037283</td>\n",
       "      <td>-0.001190</td>\n",
       "      <td>-0.010938</td>\n",
       "      <td>0.033558</td>\n",
       "      <td>deputy prime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>-0.001107</td>\n",
       "      <td>-0.082195</td>\n",
       "      <td>-0.001619</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Australian Greens  Australian Labor Party  Centre Alliance  \\\n",
       "1167          -0.012168               -0.111829        -0.002200   \n",
       "1972          -0.013101               -0.084058        -0.001108   \n",
       "1615          -0.016448               -0.010526        -0.001566   \n",
       "880           -0.015504               -0.014656        -0.001676   \n",
       "2024          -0.004280               -0.051423        -0.000895   \n",
       "682           -0.021562               -0.029050        -0.002463   \n",
       "1963          -0.008681               -0.025396        -0.001055   \n",
       "103           -0.001146               -0.028951        -0.001928   \n",
       "1299          -0.000282               -0.059610        -0.001104   \n",
       "1126          -0.027180               -0.001116        -0.001494   \n",
       "161           -0.014152               -0.012463        -0.001291   \n",
       "48            -0.009710               -0.050634        -0.001076   \n",
       "1361          -0.009187               -0.040823        -0.003085   \n",
       "1415          -0.010027               -0.046106        -0.000580   \n",
       "1864          -0.008651               -0.079430        -0.000837   \n",
       "1624          -0.003768               -0.047231        -0.000945   \n",
       "1019          -0.000704               -0.051690        -0.000333   \n",
       "537           -0.008359               -0.008282        -0.001552   \n",
       "538           -0.005054               -0.037283        -0.001190   \n",
       "1070          -0.001107               -0.082195        -0.001619   \n",
       "\n",
       "      Liberal Party of Australia  The Nationals              token  \n",
       "1167                   -0.074854       0.120708      lovegippsland  \n",
       "1972                   -0.072949       0.103549              tyfys  \n",
       "1615                   -0.045663       0.058891                 rf  \n",
       "880                    -0.030716       0.049876          gippsland  \n",
       "2024                   -0.028659       0.049726         waggawagga  \n",
       "682                    -0.001055       0.044723         everything  \n",
       "1963                   -0.028007       0.043208         tumbarumba  \n",
       "103                    -0.029873       0.042858             anyone  \n",
       "1299                   -0.017175       0.042380               near  \n",
       "1126                   -0.011814       0.041288               link  \n",
       "161                    -0.023275       0.040741          available  \n",
       "48                     -0.007069       0.040397        affect fire  \n",
       "1361                    0.002907       0.038058   opbushfireassist  \n",
       "1415                   -0.008066       0.037206              peter  \n",
       "1864                    0.013452       0.036207  telecommunication  \n",
       "1624                   -0.010833       0.035338           riverina  \n",
       "1019                   -0.014767       0.034235            imagine  \n",
       "537                    -0.022005       0.033800             deputy  \n",
       "538                    -0.010938       0.033558       deputy prime  \n",
       "1070                    0.013991       0.033430                job  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlogit.margins.sort_values(\"The Nationals\" , ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogit_ci = mlogit.bootstrap_ci(alpha=0.05, n_samples=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogit_ci.head(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd379f7bb8e06f7ec29e5b7f14bfd985640a300b5a41de1996cc2754af32f7b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
