{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.data_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we collect all the tweets from Australian MPs. To do this we rely on a list of MPs from a curated database that can be found here: http://twitterpoliticians.org/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tweepy\n",
    "from datetime import date\n",
    "import pickle \n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get twitter credentials from AppCred.py.\n",
    "#You must have your own credentials stored in working dir\n",
    "from AppCred import API_KEY, API_SECRET\n",
    "from AppCred import ACCESS_TOKEN, ACCESS_TOKEN_SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MP data and get twitter handles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we use tweepy to get the twitter \"screen name\" of each MP using the user id given by the Twitter politicians data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter by Australian Parlamentarians\n",
    "#Download the file full_member_info at http://twitterpoliticians.org/download\n",
    "mp_df = pd.read_csv(\"full_member_info.csv\")\n",
    "mp_df = mp_df[['p.country', 'm.name', 'p.party', 'm.uid']].copy()\n",
    "mp_df = mp_df.loc[mp_df[\"p.country\"]==\"Australia\"]\n",
    "filtered_df =  mp_df.loc[mp_df[\"m.uid\"] != \"\\\\N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the twitter handle of each politicians\n",
    "#and add it to the df\n",
    "def get_handle(uid):\n",
    "    try:\n",
    "        handle = api.get_user(uid).screen_name\n",
    "    except:\n",
    "        handle = np.nan\n",
    "    return handle\n",
    "            \n",
    "filtered_df[\"twitter_handle\"] = filtered_df[\"m.uid\"].apply(lambda x: get_handle(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a class that takes a list of screen names and tries to get the latest 3200 tweets. It then locally dumps the full tweepy tweet for each tweet as a dictionary. The reason we opt for storing everything is to avoid later having to call the API again in case we need more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of unique twitter handles\n",
    "handles = filtered_df[\"twitter_handle\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove private and \"nan\"\n",
    "handles.remove(\"JohnAlexanderMP\")\n",
    "handles.remove(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetCollector:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api = self.connect_api()\n",
    "    \n",
    "\n",
    "    def connect_api(self):\n",
    "        \"\"\"\n",
    "        Connect to the API upon initalizing that class. You need to have\n",
    "        your own credentials imported\n",
    "        \"\"\"\n",
    "        auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit= True, \n",
    "                         wait_on_rate_limit_notify=True, \n",
    "                         retry_count = 10, retry_delay = 5, \n",
    "                         retry_errors= set([401, 404, 500, 503]))\n",
    "        \n",
    "        return api\n",
    "        \n",
    "    def get_tweets(self, screen_name):\n",
    "        \"\"\"\n",
    "        Given a screen name, this method tries to fetch the last 3200 tweets (maximum allowed).\n",
    "        \"\"\"\n",
    "        #initialize a list to hold all the tweepy Tweets\n",
    "        all_tweets = []  \n",
    "    \n",
    "        #make initial request for most recent tweets\n",
    "        new_tweets = self.api.user_timeline(screen_name = screen_name,\n",
    "                                            count=200, tweet_mode = \"extended\")\n",
    "    \n",
    "        #save most recent tweets\n",
    "        all_tweets.extend(new_tweets)\n",
    "    \n",
    "        #save the id of the oldest tweet less one\n",
    "        oldest = all_tweets[-1].id - 1\n",
    "            \n",
    "            \n",
    "        #keep grabbing tweets until there are no tweets left to grab\n",
    "        while len(new_tweets) > 0:\n",
    "            try:\n",
    "                #all subsiquent requests use the max_id param to prevent duplicates\n",
    "                new_tweets = self.api.user_timeline(screen_name = screen_name,count=200,\n",
    "                                                    max_id=oldest, tweet_mode = \"extended\")\n",
    "\n",
    "                #save most recent tweets\n",
    "                all_tweets.extend(new_tweets)\n",
    "\n",
    "                #update the id of the oldest tweet less one\n",
    "                oldest = all_tweets[-1].id - 1\n",
    "                    \n",
    "            except tweepy.TweepError as e:\n",
    "                print(e.reason)\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "        return all_tweets\n",
    "    \n",
    "    \n",
    "    def pickle_dump(self, screen_names, dump = True):\n",
    "        \"\"\"\n",
    "        Given a list of screen names, this method returns a dictionary\n",
    "        containing all the fetchable tweets from the list of users.\n",
    "        Dumps everything as a pickle file locally\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(screen_names, list):\n",
    "            screen_names = [screen_names]\n",
    "        \n",
    "        all_tweets = {}\n",
    "        \n",
    "        for screen_name in tqdm(screen_names):\n",
    "            try:\n",
    "                tweets = self.get_tweets(screen_name)\n",
    "                all_tweets[screen_name] = tweets\n",
    "            except:\n",
    "                print(f\"could not get {screen_name}\")\n",
    "                continue\n",
    "         \n",
    "        #If True dumps all the tweets in a pickle file\n",
    "        if dump:\n",
    "            with open(f'pickled_tweets_{str(date.today())}.data', 'wb') as f:\n",
    "                # store the data as binary data stream\n",
    "                pickle.dump(all_tweets, f)\n",
    "                \n",
    "        return all_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate collector object\n",
    "collector = TweetCollector()\n",
    "#Dump tweets from all MPs\n",
    "tweets = collector.pickle_dump(handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we create a class that builds a data-frame of tweets and its corresponding relevant variables based on the locally dumped dictionary of tweepy tweet objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildTweetDF:\n",
    "    \"\"\"\n",
    "    This class builds a Pandas dataframe using a pickle dump \n",
    "    of Tweepy tweet objects as collected by the TweetCollector\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pickle_dump):\n",
    "        self.all_tweets = self.pickle_open(pickle_dump)\n",
    "    \n",
    "    def pickle_open(self, pickle_dump):\n",
    "        with open(pickle_dump, 'rb') as f:\n",
    "        # read the data as binary data stream\n",
    "            all_tweets = pickle.load(f)\n",
    "        \n",
    "        return all_tweets\n",
    "    \n",
    "    def get_df(self):\n",
    "        \n",
    "        final_df_lst = []\n",
    "        \n",
    "        for politician, tweets in self.all_tweets.items():\n",
    "            \n",
    "            #Empty list for df. More things can be added later\n",
    "            screen_name = []\n",
    "            user_id = []\n",
    "            created_at = []\n",
    "            full_text = []\n",
    "            favorite_count = []\n",
    "            retweet_count = []\n",
    "            retweet_name = []\n",
    "            tweet_id = []\n",
    "            in_reply_to_screen_name = []\n",
    "            hashtags = []\n",
    "            user_mentions = []\n",
    "            urls = []\n",
    "            image = []\n",
    "            \n",
    "            for tweet in tweets:\n",
    "                \n",
    "                screen_name.append(tweet.user.screen_name)\n",
    "                user_id.append(tweet.user.id)\n",
    "                created_at.append(tweet.created_at)\n",
    "                full_text.append(tweet.full_text)\n",
    "                favorite_count.append(tweet.favorite_count)\n",
    "                retweet_count.append(tweet.retweet_count)\n",
    "                tweet_id.append(tweet.id)\n",
    "                in_reply_to_screen_name.append(tweet.in_reply_to_screen_name)\n",
    "                \n",
    "                user_mentions.append([i[\"screen_name\"] for i in tweet.entities[\"user_mentions\"]])\n",
    "                hashtags.append([i[\"text\"] for i in tweet.entities[\"hashtags\"]])\n",
    "                try:\n",
    "                    retweet_name.append(tweet.retweeted_status.author.screen_name)\n",
    "                except:\n",
    "                    retweet_name.append(np.nan)\n",
    "                try:\n",
    "                    urls.append(tweet.entities[\"urls\"][0][\"expanded_url\"])\n",
    "                except:\n",
    "                    urls.append(np.nan)    \n",
    "                try:\n",
    "                    image.append(tweet.entities[\"media\"][0][\"media_url\"])\n",
    "                except:\n",
    "                    image.append(np.nan)\n",
    "                            \n",
    "                \n",
    "            df = pd.DataFrame({\"screen_name\":screen_name,\n",
    "                               \"user_id\":user_id,\n",
    "                               \"tweet_id\":tweet_id,\n",
    "                               \"created_at\":created_at,\n",
    "                               \"full_text\":full_text,\n",
    "                               \"favorite_count\":favorite_count,\n",
    "                               \"retweet_count\":retweet_count,\n",
    "                               \"retweet_name\":retweet_name,\n",
    "                               \"in_reply_to_screen_name\":in_reply_to_screen_name,\n",
    "                               \"hashtags\":hashtags,\n",
    "                               \"user_mentions\":user_mentions,\n",
    "                               \"url\":urls,\n",
    "                               \"image_url\":image})\n",
    "            \n",
    "            #Append politican df to list of all dfs\n",
    "            final_df_lst.append(df)\n",
    "            \n",
    "        #Concat to one final df\n",
    "        final_df = pd.concat(final_df_lst).reset_index(drop=True)\n",
    "        \n",
    "        return final_df\n",
    "                \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a pickle dump\n",
    "build_df = BuildTweetDF(\"data/pickled_tweets_2021-05-04.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = build_df.get_df()\n",
    "#tweet_df.to_csv(\"mp_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv(\"data/mp_tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
