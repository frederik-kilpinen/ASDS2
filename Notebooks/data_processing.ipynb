{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/frederik-kilpinen/ASDS2/blob/main/Notebooks/data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Wrangling and Mergin\n",
    "2. Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wranling and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K5jNeH3d9cqY"
   },
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tweepy\n",
    "from datetime import date\n",
    "import pickle \n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8_kMAvdu9cqb",
    "outputId": "b54884f6-b012-498c-d30c-c5e4f42f0be7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15918, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MP info dataset\n",
    "mp_df = pd.read_csv(\"data/full_member_info.csv\")\n",
    "\n",
    "#Original shape of the data\n",
    "mp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nAK3IMzd9cqc",
    "outputId": "061ce6d8-ec6e-4b8d-bbfc-fe8c37cbbf70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335975, 12)\n"
     ]
    }
   ],
   "source": [
    "# Twitter dataset\n",
    "tweets = pd.read_csv(\"data/mp_tweets\", index_col=0, low_memory=False)\n",
    "\n",
    "#Original shape of the data\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LiXo-A0o9cqd"
   },
   "outputs": [],
   "source": [
    "class DataProcessing:\n",
    "\n",
    "    def clean_tweet_data(self, tweet_df):\n",
    "\n",
    "\n",
    "        #Drop 6 tweets that are corrupt. Because of it only being 6 tweets we drop them instead of re-running the collection from the API\n",
    "        remove_idx = [175522, 190414, 211953, 212012, 212013, 212298 ]\n",
    "        tweet_df = tweet_df.drop(tweet_df.index[remove_idx])\n",
    "\n",
    "        #Make data into date-time object, remove h-m-s from dt\n",
    "        tweet_df[\"created_at\"] = pd.to_datetime(pd.to_datetime(tweet_df[\"created_at\"]).dt.date)\n",
    "        \n",
    "        tweet_df[\"user_id\"] = tweet_df[\"user_id\"].astype(int)\n",
    "        \n",
    "        return tweet_df\n",
    "    \n",
    "    def clean_mp_data(self, mp_df):\n",
    "        \n",
    "        \n",
    "        \n",
    "        mp_df = mp_df[['p.country', 'm.name', 'p.party', 'm.uid', 'lp.official_legislative_period']]\n",
    "        mp_df = mp_df.loc[mp_df[\"p.country\"]==\"Australia\"]\n",
    "        \n",
    "        #Drop australia column\n",
    "        mp_df = mp_df.drop(columns = [\"p.country\"])\n",
    "        #Rename some columns\n",
    "        mp_df = mp_df.rename(columns = {\"m.name\":\"name\", \"p.party\":\"party\",\n",
    "                                       \"lp.official_legislative_period\":\"legislative_period\"})\n",
    "        \n",
    "        #Rename user id column for merging with members_info data\n",
    "        mp_df = mp_df.rename(columns = {\"m.uid\":\"user_id\"})\n",
    "        \n",
    "        \n",
    "        remove = r\"(^[A-Za-z]{2}\\s{1}|\\s{1}[A-Z]{2,}|^Hon\\s{1}|^Mrs\\s{1}|(Dr\\s)|,)\"\n",
    "        mp_df[\"name\"] = mp_df[\"name\"].str.replace(remove, \"\", regex = True)\n",
    "        \n",
    "        mp_df = mp_df.loc[mp_df[\"user_id\"] != \"\\\\N\"]\n",
    "        mp_df[\"user_id\"] = mp_df[\"user_id\"].astype(int)\n",
    "        \n",
    "        # Merge the Nick Xenophon Team and Centre Alliance \n",
    "        mp_df[\"party\"] = mp_df[\"party\"].apply(lambda x: \"Centre Alliance\" if x == \"Nick Xenophon Team\" else x)\n",
    "        \n",
    "        \n",
    "\n",
    "        return mp_df\n",
    "    \n",
    "    def merge_final_df(self, tweet_df, mp_df):\n",
    "        \n",
    "        \n",
    "        tweet_df = self.clean_tweet_data(tweet_df)\n",
    "        mp_df = self.clean_mp_data(mp_df)\n",
    "        \n",
    "        #Merge to final df\n",
    "        final_df = tweet_df.merge(mp_df, on = \"user_id\", how = \"left\")\n",
    "        \n",
    "        #Subset on active MPs\n",
    "        final_df = final_df.loc[((final_df[\"legislative_period\"] == \"45\") & (final_df[\"created_at\"] < \"2019-07-01\"))|\n",
    "                                ((final_df[\"legislative_period\"] == \"46\") & (final_df[\"created_at\"] > \"2019-07-01\"))]\n",
    "        \n",
    "        # FREDERIKS CHANGE: Subset tweets from 1 year before the bushfire (1. June 2018) and 1 year after the bushfire (1. May 2021)\n",
    "        final_df = final_df.loc[(final_df[\"created_at\"] >= \"2018-06-01\") & (final_df[\"created_at\"] <= \"2021-04-30\")]\n",
    "           \n",
    "        # Restetting index for final df\n",
    "        final_df = final_df.reset_index(drop = True)\n",
    "        \n",
    "        return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OiQmYnQI9cqe",
    "outputId": "fba3aa0e-4485-4a66-95c5-bea11b91710d"
   },
   "outputs": [],
   "source": [
    "processor = DataProcessing()\n",
    "final_df = processor.merge_final_df(tweets, mp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"final_tweet_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170338, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#Reusing and tweaking the function for preprocessing from last week to fit specifics of this dataset.\n",
    "def preprocess(text):\n",
    "    \n",
    "    #Lowercasing words\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Removing '&amp' which was found to be common\n",
    "    text = re.sub(r'&amp','', text)\n",
    "    \n",
    "    #Replace other instances of \"&\" with \"and\"\n",
    "    text = re.sub(r'&','and', text)\n",
    "    \n",
    "    #Removing mentions \n",
    "    text = re.sub(r'@\\w+ ', '', text)\n",
    "    \n",
    "    #Removing 'RT' and 'via'\n",
    "    text = re.sub(r'(^rt|^via)((?:\\b\\W*@\\w+)+): ', '', text)\n",
    "    \n",
    "    #Removing punctuation\n",
    "    my_punctuation = string.punctuation.replace('#','')\n",
    "    my_punctuation = my_punctuation.replace('-','')\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', my_punctuation))\n",
    "    text = re.sub(r' - ','', text) #removing dash lines bounded by whitespace (and therefore not part of a word)\n",
    "    text = re.sub(r'[’“”—,!]','',text) #removing punctuation that is not captured by string.punctuation\n",
    "    \n",
    "    #Removing odd special characters\n",
    "    text = re.sub(r\"[┻┃━┳┓┏┛┗]\",\"\", text)\n",
    "    text = re.sub(r\"\\u202F|\\u2069|\\u200d|\\u2066\",\"\", text)\n",
    "    \n",
    "    #Removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #Removing numbers\n",
    "    text = re.sub(r'[0-9]','', text)\n",
    "    \n",
    "    #Removing separators and superfluous whitespace\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +',' ',text)\n",
    "    \n",
    "    #Tokenizing\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def preprocess_lemma(tokens):\n",
    "    \n",
    "    #Running the preprocess function\n",
    "    tokens = preprocess(tokens)\n",
    "    \n",
    "    #Lemmatizing\n",
    "    tag_map = defaultdict(lambda : nltk.corpus.wordnet.NOUN)      #POS map\n",
    "    tag_map['J'] = nltk.corpus.wordnet.ADJ\n",
    "    tag_map['V'] = nltk.corpus.wordnet.VERB\n",
    "    tag_map['R'] = nltk.corpus.wordnet.ADV    \n",
    "    \n",
    "    lemmatizer = nltk.WordNetLemmatizer()             #Creating lemmatizer.\n",
    "    text_lemmatized = []                              #Empty list to save lemmatized sentence\n",
    "\n",
    "    for word, tag in nltk.pos_tag(tokens):\n",
    "        lemma = lemmatizer.lemmatize(word, tag_map[tag[0]])\n",
    "        text_lemmatized.append(lemma)\n",
    "    \n",
    "    tokens = text_lemmatized\n",
    "\n",
    "    #Removing stopwords\n",
    "    stop_words_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    text = \" \".join([i for i in tokens if i not in stop_words_list])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_stem(tokens):\n",
    "    \n",
    "    #Running the preprocess function\n",
    "    tokens = preprocess(tokens)\n",
    "    \n",
    "    #Removing stopwords\n",
    "    stop_words_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [i for i in tokens if i not in stop_words_list]\n",
    "    \n",
    "    #Stemming\n",
    "    stemmer = nltk.PorterStemmer()    #Creating stemmer\n",
    "    sent_stemmed = []                 #Empty list to save stemmed sentence\n",
    "    \n",
    "    for word in tokens:\n",
    "        stem = stemmer.stem(word)     #Stemming words\n",
    "        sent_stemmed.append(stem)\n",
    "        \n",
    "    tokens = sent_stemmed\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lemmatized column\n",
    "\n",
    "final_df[\"lemma_text\"] = final_df[\"full_text\"].apply(lambda x: preprocess_lemma(x))\n",
    "final_df[\"stemmed_text\"] = final_df[\"full_text\"].apply(lambda x: preprocess_stem(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the nan values with empty strings\n",
    "final_df.lemma_text = final_df.lemma_text.apply(lambda x: \"\" if str(x) == \"nan\" else x)\n",
    "final_df.stemmed_text = final_df.stemmed_text.apply(lambda x: \"\" if str(x) == \"nan\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add additional lemma and stemmed columns with only nouns proper nouns and verbs\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_bos(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [ word for (word,tag) in blob.tags if tag in [\"NN\", \"NNP\", \"VD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df[\"lemma_text_sub\"] = final_df[\"lemma_text\"].apply(lambda x: get_bos(x))\n",
    "final_df[\"stemmed_text_sub\"] = final_df[\"stemmed_text\"].apply(lambda x: get_bos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"data/final_df_lemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "data_processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
